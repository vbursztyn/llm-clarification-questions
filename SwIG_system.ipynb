{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZpwBpJVs2WV8",
        "ojlPwz7TGvxd",
        "x7CeFKIQ3yIL",
        "lsb2udidn1NM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Possible Paper Titles\n",
        "\n",
        "1) Improving LLM conversational recommendations: How Empathy Leads to Information Gain\n",
        "\n",
        "2) SwIG: Sampling with Information Gain to improve conversational recommendations"
      ],
      "metadata": {
        "id": "CnLFBHTP1kea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to Drive and Loading Requirements\n"
      ],
      "metadata": {
        "id": "ZpwBpJVs2WV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4uSeYoA1qaY",
        "outputId": "265a8ced-193b-42d7-ed8a-fdfd75f741a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_dir = '/content/drive/MyDrive/Clarifying_Questions_GPT_Research'"
      ],
      "metadata": {
        "id": "dXxVUm8D150e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd {drive_dir} && pip install -r requirements_2.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34bZLvmV17Lk",
        "outputId": "a7daf632-dd5b-4d6f-e27c-96c7d0ee19e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 2)) (0.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 3)) (1.23.5)\n",
            "Collecting openai (from -r requirements_2.txt (line 4))\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 5)) (1.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 6)) (9.4.0)\n",
            "Collecting python-json-logger (from -r requirements_2.txt (line 7))\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 8)) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 9)) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 10)) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline->-r requirements_2.txt (line 2)) (5.7.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements_2.txt (line 4)) (3.8.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements_2.txt (line 5)) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements_2.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements_2.txt (line 8)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements_2.txt (line 8)) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements_2.txt (line 8)) (2023.7.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements_2.txt (line 9)) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements_2.txt (line 9)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements_2.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements_2.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (1.3.1)\n",
            "Installing collected packages: python-json-logger, openai\n",
            "Successfully installed openai-0.28.1 python-json-logger-2.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "import pickle\n",
        "\n",
        "import json\n",
        "\n",
        "import threading\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "\n",
        "import re\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import math\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import copy\n",
        "\n",
        "import signal\n",
        "\n",
        "import time\n",
        "\n",
        "import sys\n",
        "\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pprint\n",
        "\n",
        "import urllib3"
      ],
      "metadata": {
        "id": "DQObQKqP3fn_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(drive_dir+\"/openai_key.key\", \"r\") as f:\n",
        "    openai.api_key = f.read()"
      ],
      "metadata": {
        "id": "UZOZNb1tnmDs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Functions and Decorators"
      ],
      "metadata": {
        "id": "ojlPwz7TGvxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# given a string that has a list, creates a python list with the input's contents\n",
        "def str2lst(l):\n",
        "\n",
        "  # if it is a pyhton list as a str\n",
        "  if '[' in l and ']' in l:\n",
        "    # removing double spaces and newlines\n",
        "    l = re.sub(\"\\s+\", \" \", l)\n",
        "    # removing anything that isn't the list\n",
        "    m = re.search('\\[(.|\\s)+\\]', l)\n",
        "    l = l[m.start():m.end()]\n",
        "    # converting string to list\n",
        "    l = l.strip('][')\n",
        "    l = re.split(\"\"\"['\"], \"\"\", l)\n",
        "\n",
        "    # spaghetti code to dealing with this weird bug I saw before\n",
        "    if len(l) == 1:\n",
        "      l = l[0]\n",
        "      l = re.sub(\"-\", \"\", l)\n",
        "      l = l.split('\\n')\n",
        "\n",
        "  # else if it is bullet points tha are...\n",
        "  #numbered\n",
        "  elif re.search('\\d\\.', l):\n",
        "    # converting string to list\n",
        "    l = re.findall('\\d\\.\\s(.+)', l)\n",
        "\n",
        "  #not numbered\n",
        "  else:\n",
        "    # converting string to list\n",
        "    l = re.split('-', l)[1:]\n",
        "\n",
        "  # removing extra quotation marks\n",
        "  for i in range(len(l)): l[i] = re.sub(\"\"\"['\"]\"\"\", \"\", l[i])\n",
        "  # removing trailing and starting spaces\n",
        "  for i in range(len(l)): l[i] = l[i].strip(' ')\n",
        "\n",
        "  return l\n",
        "\n",
        "# function to turn a python list into a numbered list as a string\n",
        "def list2numbered(lst):\n",
        "  num_lst = []\n",
        "  for idx in range(len(lst)): num_lst.append(f'{idx+1}. {lst[idx]}\\n')\n",
        "  return ''.join(num_lst)\n",
        "\n",
        "# decorator that saves the output of func into a specific index of a lst (for multi threading)\n",
        "def saveInLst(lst, idx):\n",
        "\n",
        "  def inner(func):\n",
        "\n",
        "    def wrap(*args, **kwargs):\n",
        "      lst[idx] = func(*args, **kwargs)\n",
        "      return\n",
        "\n",
        "    return wrap\n",
        "\n",
        "  return inner\n",
        "\n",
        "# given a function, a list of input lists, creates a thread for each input list, running the function with different inouts in parallel. Returns a list of outputs\n",
        "def multiThread(func, input_lst):\n",
        "  thread_n = len(input_lst)\n",
        "  threads = []\n",
        "\n",
        "  results = [None for i in range(thread_n)]\n",
        "\n",
        "  n = 80\n",
        "  idxs = [i for i in range(thread_n )]\n",
        "  batch_idxs = [idxs[i * n:(i + 1) * n] for i in range((len(idxs) + n - 1) // n )]\n",
        "  # print(batch_idxs)\n",
        "  for b in batch_idxs:\n",
        "    # print(b)\n",
        "    for i in b:\n",
        "      time.sleep(2) # my attempt to stop hitting the rate limit\n",
        "      tr = threading.Thread(target=saveInLst(results, i)(func), args=(*input_lst[i],))\n",
        "      # tr = threading.Thread(target=func, args=(*input_lst[i],))\n",
        "      tr.start()\n",
        "      threads.append(tr)\n",
        "\n",
        "    for tr in threads:\n",
        "      tr.join()\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "1PYfBrn-Gwh0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for Text Generation"
      ],
      "metadata": {
        "id": "x7CeFKIQ3yIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generates text using gpt-3.5 given a conversation context, returns None if its not able to generate text after a certain number of tries\n",
        "def generateText(context, temp = 0.7, tries = 3, max_time = 120):\n",
        "  assert isinstance(context, list), f'input is not a context list, got {type(context)}'\n",
        "\n",
        "  resp = None\n",
        "\n",
        "  while tries != 0:\n",
        "    try:\n",
        "      resp = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=context,\n",
        "        temperature=temp)['choices'][0]['message']['content']\n",
        "\n",
        "      tries = 0\n",
        "    except:\n",
        "      tries -= 1\n",
        "\n",
        "  return resp\n",
        "\n",
        "\n",
        "# def generateMText(context, thread_n, temp = 0.7, tries = 3):\n",
        "#   threads = []\n",
        "\n",
        "#   results = [None for i in range(thread_n)]\n",
        "\n",
        "#   seed = 'Generate a profile of a user who is looking to travel. The profile should be comprised of a few bullet points that describe them'\n",
        "\n",
        "#   for i in range(thread_n):\n",
        "#     tr = threading.Thread(target=saveInLst(results, i)(generateText), args=(context, temp, tries,))\n",
        "#     tr.start()\n",
        "#     threads.append(tr)\n",
        "\n",
        "#   for tr in threads:\n",
        "#     tr.join()\n",
        "\n",
        "#   return results\n",
        "\n",
        "# appends a new entry to a conversation context\n",
        "def appendContext(text, context, role = 'user'):\n",
        "  assert role == 'user' or role == 'assistant', f'unexpected role, got: {role}'\n",
        "\n",
        "  if not context:\n",
        "    context.append({'role': role, 'content': text})\n",
        "    return\n",
        "\n",
        "  if role == 'user':\n",
        "    assert context[-1]['role'] == 'assistant', f'incompatible adjecent role, got user'\n",
        "  else:\n",
        "    assert context[-1]['role'] == 'user', f'incompatible adjecent role, got assistant'\n",
        "\n",
        "  context.append({'role': role, 'content': text})\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "LtqU76cG3xZe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts"
      ],
      "metadata": {
        "id": "tlvTRnk4ZqXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stichPrompt(prompt, var_lst):\n",
        "  var_places = len(re.findall('{\\S*}', prompt))\n",
        "  assert var_places == len(var_lst), f'prompt has to have the same number of places for variables as variables in var_lst. Prompt has {var_places} var places and len(var_lst) = {len(var_lst)}. Here is the prompt:\\n{prompt}'\n",
        "  for var_idx in range(var_places):\n",
        "    m = re.search('{\\S*}', prompt)\n",
        "    start = prompt[:m.start()]\n",
        "    end = prompt[m.end():]\n",
        "    prompt = f'{start}{var_lst[var_idx]}{end}'\n",
        "\n",
        "  return prompt\n",
        "\n",
        "# CREATE CODE THAT SERVES AS DICT OF ALL PROPMTS (WHERE THE VALUES AZRE FUNCTIONS)\n",
        "stichPrompt(\"Here is an example prompt, with a var here: {var} and here: {}\", [1,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OiVngHxyJd3L",
        "outputId": "91492995-e6e2-490f-c63f-1e14f4856cd8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is an example prompt, with a var here: 1 and here: 2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = {'startConv': [\"\"\"You are taking an English test.\\nHere is a sentence: \"{i_want_sen}, but I do not know [insert word]. Help me by asking me a question at a time.\"\\nReturn the sentence but with the correct word filled in the empty slot:\"\"\"],\n",
        "\n",
        "           'genRealProfile': [\"Generate a profile of a user. The profile should be a list of keywords about their preferences regarding {domain}.\"],\n",
        "\n",
        "           'continueConvBool': [\"Is the following text a question or a suggestion?/n/n{text}/n/nAnswer [Question, Suggestion]:\"],\n",
        "\n",
        "           'genUProfiles': ['Generate {number} different profiles. Each profile should be one sentence containing a sequence of keywords about their preferences regarding {domain}.'],\n",
        "\n",
        "           'extractLastPreference': ['Here is a conversation.\\nASSISTANT:\\n{question}\\nUSER:\\n{answer}\\nTASK: Summarize what we know about the user in a single sentence.\\nSUMMARY:'],\n",
        "\n",
        "           'genYProfiles': ['Here is a list of user preferences:{bullet_points}',\n",
        "                            'Here is a sentence describing the preferences of a user:{sentence}\\n',\n",
        "                            'Create {number} different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:',\n",
        "                            'Create different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'],\n",
        "\n",
        "           'answerQ': ['You are a character. Here is what you know about your character: {profile}\\nYou are speaking to an assistant and you speak in brief sentences.\\nAnswer the assistant in character.\\nAssistant: {question}\\nYou:'],\n",
        "\n",
        "           'pickBranch': [\"\"\"You are the following character: {profile}\\nYou are asked the following question: {question}\\n\\nFrom the options below, which is most likely your answer to the question?\\n{bullet_points}\\nRemember, you have to pick one from the list or answer \"None of the answers in the list\". Only return the most likely answer, followed by the number that is the place of the answer in the list.\\nAnswer, Number:\"\"\",\n",
        "                          \"You are the following character: {profile}\\nYou are asked the following question: {question}\\nIs it likely that you would answer: {answer}?\\n\\n[Yes, No]:\"],\n",
        "\n",
        "          'makeBranches': ['Here is a question:\\n\"{question}\"\\nReturn the smallest numbered list of different possible answers to this question. Make sure that this list, althought small, covers how any user could answer this question.'],\n",
        "\n",
        "           'makeNodes': ['Generate a python list of {node_n} questions you could possibly ask me at this point of the conversation.',\n",
        "                         'Generate a python list of questions you could possibly ask me at this point of the conversation.',\n",
        "                         ' Inlcude the last question you just asked in the python list.',\n",
        "                         \"Let's say I said the following: {answer}\\n {prompt}\"],\n",
        "\n",
        "\n",
        "           }"
      ],
      "metadata": {
        "id": "YJdxUfJ7CAeq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for Profiles"
      ],
      "metadata": {
        "id": "UTSV5_KrZxY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create U Profiles\n",
        "def genUProfiles(prof_num = 50, domain = 'movies'):\n",
        "  prompt = stichPrompt(prompts['genUProfiles'][0], [prof_num, domain])\n",
        "  #prompt = f'Generate {prof_num} different profiles. Each profile should be one sentence containing a sequence of keywords about their preferences regarding traveling.'\n",
        "  return [Profile(txt, ProfileType.U) for txt in str2lst(generateText([{'role': 'user', 'content': prompt}], tries = 2))]\n",
        "\n",
        "# Extract the last preference the user revealed in the chat context\n",
        "def extractLastPreference(chat_hist):\n",
        "  chat_hist = chat_hist[-2:]\n",
        "  assert chat_hist[0]['role'] == 'assistant', 'chat history should have the assistant as the second to last message'\n",
        "  assert chat_hist[1]['role'] == 'user', 'chat history should have the user as the last message'\n",
        "\n",
        "  prompt = stichPrompt(prompts['extractLastPreference'][0], [chat_hist[0]['content'], chat_hist[1]['content']])\n",
        "  # prompt = f'Here is a conversation.\\nASSISTANT:\\n{chat_hist[0]['content']}\\nUSER:\\n{chat_hist[1]['content']}\\nTASK: Summarize what we know about the user in a single sentence.\\nSUMMARY:'\n",
        "\n",
        "  return generateText([{'role': 'user', 'content': prompt}], tries = 2)\n",
        "\n",
        "# Create Y Profiles\n",
        "def genYProfiles(preference_lst = [], prof_num = None):\n",
        "  p_n = len(preference_lst)\n",
        "  assert p_n != 0, 'need at least one preference in preference_lst'\n",
        "\n",
        "  prompt = preference_lst[0]\n",
        "\n",
        "  # if preference_lst has more than oone preferece, create a single sentence representing all of them\n",
        "  if p_n > 1:\n",
        "    prompt = stichPrompt(prompts['genYProfiles'][0], [list2numbered(preference_lst)])\n",
        "    # prompt = f'Here is a list of user preferences:{list2numbered(preference_lst)}'\n",
        "    prompt = generateText([{'role': 'user', 'content': prompt+'Create a single sentence of key words that describes all of these preferences:'}], tries = 2)\n",
        "\n",
        "  prompt = stichPrompt(prompts['genYProfiles'][1], [prompt])\n",
        "  # prompt = f'Here is a sentence describing the preferences of a user:{prompt}\\n'\n",
        "\n",
        "  # if profile number is specified, generate this specific number, otherwise let LLM decide\n",
        "  if prof_num:\n",
        "    prompt = stichPrompt(prompts['genYProfiles'][2], [prof_num])\n",
        "    # prompt += f'Create {prof_num} different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'\n",
        "  else:\n",
        "    prompt += prompts['genYProfiles'][3]\n",
        "    # prompt += 'Create different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'\n",
        "\n",
        "  return [Profile(txt, ProfileType.Y) for txt in str2lst(generateText([{'role': 'user', 'content': prompt}], tries = 2))]\n",
        "\n",
        "# Create a real profile\n",
        "def genRealProfile(domain = 'movies'):\n",
        "  prompt = stichPrompt(prompts['genRealProfile'][0], [domain])\n",
        "  #prompt = f'Generate {prof_num} different profiles. Each profile should be one sentence containing a sequence of keywords about their preferences regarding traveling.'\n",
        "  return Profile(generateText([{'role': 'user', 'content': prompt}], tries = 2), ProfileType.R)"
      ],
      "metadata": {
        "id": "7Y78U4RxHbDB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree Structure"
      ],
      "metadata": {
        "id": "4haEyKYJZ53U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enum of Profile types\n",
        "class ProfileType(Enum):\n",
        "  U = 0\n",
        "  Y = 1\n",
        "  R = 2\n",
        "\n",
        "# Class representing a user profile\n",
        "class Profile:\n",
        "  def __init__(self, text: str, p_type: ProfileType):\n",
        "    self.text = text\n",
        "    self.p_type = p_type\n",
        "    return\n",
        "\n",
        "  # # makes this Proifle into a UProfile\n",
        "  # def makeUProfile(self):\n",
        "  #   self.p_type = ProfileType.U\n",
        "  #   return\n",
        "\n",
        "  # # makes this Proifle into a YProfile\n",
        "  # def makeYProfile(self, context):\n",
        "  #   self.p_type = ProfileType.Y\n",
        "  #   return\n",
        "\n",
        "  # answers question q as the user the profile describes, returns string\n",
        "  @staticmethod\n",
        "  def answerQ(self, q):\n",
        "    prompt = stichPrompt(prompts['answerQ'][0], [self.text, q])\n",
        "    # prompt = f'You are a character. Here is what you know about your character: {self.text}\\nYou are speaking to an assistant and you speak in brief sentences.\\nAnswer the assistant in character.\\nAssistant: {q}\\nYou:'\n",
        "    return generateText([{'role': 'user', 'content': prompt}])\n",
        "\n",
        "  # picks branch that best fits profile, returns int that is index of picked branch in the branches list or None\n",
        "  @staticmethod\n",
        "  def pickBranch(self, node):\n",
        "    prompt = stichPrompt(prompts['pickBranch'][0], [self.text, node.question, list2numbered(node.branches)])\n",
        "    # prompt = f\"\"\"You are the following character: {self.text}\\nYou are asked the following question: {node.question}\\n\\nFrom the options below, which is most likely your answer to the question?\\n{list2numbered(node.branches)}\\nRemember, you have to pick one from the list or answer \"None of the answers in the list\". Only return the most likely answer, followed by the number that is the place of the answer in the list.\\nAnswer, Number:\"\"\"\n",
        "    txt = generateText([{'role': 'user', 'content': prompt}], temp=1.0)\n",
        "\n",
        "    # trying to find a number in the answer generated\n",
        "    n_m = re.search('\\d', txt)\n",
        "\n",
        "    if n_m:\n",
        "      # if the number chosen is not an index to the branches list, return None\n",
        "      branch_idx = int(txt[n_m.start():n_m.end()]) - 1\n",
        "      if branch_idx >= len(node.branches):\n",
        "        return\n",
        "\n",
        "      # if given an appropriate answer, make sure the answer shouldn't be neither\n",
        "      prompt = stichPrompt(prompts['pickBranch'][1], [self.text, node.question, node.branches[branch_idx]])\n",
        "      # prompt = f\"You are the following character: {self.text}\\nYou are asked the following question: {node.question}\\nIs it likely that you would answer: {txt[t_m.start():t_m.end()]}?\\n\\n[Yes, No]:\"\n",
        "\n",
        "      if 'yes' in generateText([{'role': 'user', 'content': prompt}]).lower():\n",
        "        return branch_idx\n",
        "\n",
        "    # if no number was given, or if the number was not an appropriate answer, return None\n",
        "    return\n",
        "\n",
        "# Class representing a node in a layer of the \"decision tree\"\n",
        "class Node:\n",
        "  def __init__(self, q : str):\n",
        "    # question that represents this node\n",
        "    self.question = q\n",
        "    # possible branches/answers to this node/question\n",
        "    self.branches = None\n",
        "    self.tst_b = None\n",
        "    # map of profile index (from given profile_lst input in splitProfiles func) to branch index\n",
        "    self.profile_branch_map = None\n",
        "    # entropy and info gain for this Node\n",
        "    self.entropy = None\n",
        "    self.info_gains = None\n",
        "\n",
        "    return\n",
        "\n",
        "  # generates branches for this node\n",
        "  @staticmethod\n",
        "  def makeBranches(self):\n",
        "    assert self.question != None, \"self.question cannot be None\"\n",
        "\n",
        "    prompt = stichPrompt(prompts['makeBranches'][0], [self.question])\n",
        "    # prompt = f'Here is a question:\\n\"{self.question}\"\\nReturn a concise list of different possible answers to the question.'\n",
        "    self.tst_b = generateText([{'role': 'user', 'content': prompt}], tries = 2, temp = 1.0)\n",
        "    self.branches = str2lst(self.tst_b)\n",
        "    return\n",
        "\n",
        "  # splits profiles into the different branches, then creates a mapping from profile index to branch index\n",
        "  @staticmethod\n",
        "  def splitProfiles(self, profile_lst : List[Profile]):\n",
        "    assert self.branches != None, \"self.branches cannot be None\"\n",
        "    assert len(profile_lst) != 0, \"self.profile_lst cannot be empty\"\n",
        "\n",
        "    # each profile picks branch that is best fit, returns list of int\n",
        "    # saving what profile picked what branch in self.profile_branch_map\n",
        "    self.profile_branch_map = multiThread(Profile.pickBranch, [[p, self] for p in profile_lst])\n",
        "\n",
        "    # editing self.branches and self.profile_branch_map if a profile picked no Branches\n",
        "    has_none = False\n",
        "    for idx in range(len(self.profile_branch_map)):\n",
        "      if self.profile_branch_map[idx] == None:\n",
        "        self.profile_branch_map[idx] = len(self.branches)\n",
        "        has_none = True\n",
        "\n",
        "    if has_none:\n",
        "      self.branches.append(\"NONE OF THE ABOVE\")\n",
        "\n",
        "    return\n",
        "\n",
        "  # calculating info gain\n",
        "  def clalcInfoGain(self, profile_lst : List[Profile]):\n",
        "    # number of total profiles\n",
        "    total_profiles_n = len(profile_lst)\n",
        "    # creating a set of classes\n",
        "    classes = set()\n",
        "    for p in profile_lst:\n",
        "      classes.add(p.p_type)\n",
        "    # map from branch index to count of profiles that picked this branch\n",
        "    branch_count = Counter(self.profile_branch_map)\n",
        "    # map from branch index to possible node entropy\n",
        "    branch_entropy = []\n",
        "\n",
        "    # if there is only one class, treat every profile as its own class\n",
        "    if len(classes) == 1:\n",
        "      # calculating this node's entropy\n",
        "      self.entropy = -1*math.log2(1/total_profiles_n)\n",
        "\n",
        "      # calculating the entropy of each branch\n",
        "      for b_idx in range(len(self.branches)):\n",
        "        if branch_count[b_idx] != 0: branch_entropy.append(-1*math.log2(1/branch_count[b_idx]))\n",
        "        else: branch_entropy.append(0)\n",
        "\n",
        "    # otherwise\n",
        "    else:\n",
        "      # calculating this node's entropy\n",
        "      self.entropy = 0\n",
        "      for c in Counter([p.p_type.value for p in profile_lst]).values(): self.entropy -= (c/total_profiles_n)*math.log2(c/total_profiles_n)\n",
        "\n",
        "      # calculating the entropy of each branch\n",
        "      assert 1==0\n",
        "\n",
        "    # calculating information gain\n",
        "    branch_entropy_weighted_sum = 0\n",
        "    for b_idx in range(len(self.branches)):\n",
        "      branch_entropy_weighted_sum += (branch_count[b_idx]/total_profiles_n) * branch_entropy[b_idx]\n",
        "\n",
        "    self.info_gains = self.entropy - branch_entropy_weighted_sum\n",
        "    return\n",
        "\n",
        "\n",
        "# Class representing a layer of the \"decision tree\"\n",
        "class Layer:\n",
        "  def __init__(self, context):\n",
        "    # current context\n",
        "    self.context = context\n",
        "    # list of nodes, each representing a possible question for this layer of the conversation\n",
        "    self.nodes = None\n",
        "\n",
        "    # list of Profile\n",
        "    self.profiles = []\n",
        "\n",
        "    # index of the node with the highest information gain\n",
        "    self.best_node_idx = None\n",
        "\n",
        "    # stores next layer\n",
        "    self.next = None\n",
        "    # stores previous layer\n",
        "    self.prev = None\n",
        "\n",
        "    return\n",
        "\n",
        "  # creating U Profiles\n",
        "  def makeUProfiles(self):\n",
        "    return\n",
        "\n",
        "  # creating nodes, hence possible questions for this layer\n",
        "  def makeNodes(self, node_n = None):\n",
        "    # making a deep copy of the context so it is unchanged\n",
        "    context = copy.deepcopy(self.context)\n",
        "\n",
        "    # making the base prompt\n",
        "    if node_n:\n",
        "      prompt = stichPrompt(prompts['makeNodes'][0], [node_n])\n",
        "      # prompt = f'Generate a python list of {node_n} questions you could possibly ask me at this point of the conversation.'\n",
        "    else:\n",
        "      prompt = prompts['makeNodes'][1]\n",
        "      # prompt = 'Generate a python list of questions you could possibly ask me at this point of the conversation.'\n",
        "\n",
        "    if context[-1]['role'] == 'assistant':\n",
        "      prompt += prompts['makeNodes'][2]\n",
        "      # prompt += ' Inlcude the last question you just asked in the python list.'\n",
        "    else:\n",
        "      prompt = stichPrompt(prompts['makeNodes'][3], [context.pop()['content'], prompt])\n",
        "      # prompt = f\"Let's say I said the following: {context.pop()['content']}\\n {prompt}\"\n",
        "\n",
        "\n",
        "    # appending the prompt to context\n",
        "    appendContext(prompt, context, 'user')\n",
        "\n",
        "    # generating the questions and storing them as a python list, then initializing nodes\n",
        "    self.nodes = [Node(q) for q in str2lst(generateText(context, tries = 2))]\n",
        "    return\n",
        "\n",
        "  # creates branches for each Node in self.nodes\n",
        "  def makeBranches(self):\n",
        "    assert self.nodes != None, \"self.nodes cannot be None\"\n",
        "    multiThread(Node.makeBranches, [[n] for n in self.nodes])\n",
        "    return\n",
        "\n",
        "  # create a mapping from profiles to branches for each node\n",
        "  def splitProfiles(self):\n",
        "    assert self.nodes != None, \"self.nodes cannot be None\"\n",
        "    assert len(self.profiles) != 0, \"self.profiles cannot be empty\"\n",
        "    multiThread(Node.splitProfiles, [[n, self.profiles] for n in self.nodes])\n",
        "    return\n",
        "    # for n in self.nodes:\n",
        "    #   n.splitProfiles(self.profiles)\n",
        "    # return\n",
        "\n",
        "  # calculates info gain for every node and saves index of best node\n",
        "  def calcInfoGain(self):\n",
        "    for n in self.nodes: n.clalcInfoGain(self.profiles)\n",
        "    ig = [n.info_gains for n in self.nodes]\n",
        "    self.best_node_idx = ig.index(max(ig))\n",
        "\n",
        "    return\n",
        "\n",
        "  # ## NEED TO IMPLEMENT THIS FOR NEW CONCENPTION OF NODE (AS WELL AS ALL THE OTHER FUNCTIONS)\n",
        "  # # creates branches using profiles by calling the ProfileGroup pickbranch(self, node) method for the ProfileGroup in index profile_group_idx in self.profile_groups\n",
        "  # def makeBranchesP(self, profile_group_idx = None):\n",
        "  #   if profile_group_idx and len(self.profiles) <= profile_group_idx:\n",
        "  #     print(f'self.profiles does not have index {profile_group_idx}')\n",
        "  #     return\n",
        "\n",
        "  #   if profile_group_idx and len(self.profiles[profile_group_idx]) == 0:\n",
        "  #     print(f'no profiles exists in index {profile_group_idx }')\n",
        "  #     return\n",
        "\n",
        "  #   if profile_group_idx:\n",
        "  #     b_maker_ps = self.profiles[profile_group_idx]\n",
        "  #   else:\n",
        "  #     b_maker_ps = [p for p_group in self.profiles for p in p_group]\n",
        "\n",
        "  #   self.b = multiThread(Profile.answerQ, [[p, self.q] for p in b_maker_ps])\n",
        "  #   return\n",
        "\n",
        "  # # aggregate branches that are the same, sets self.b to a list of strings GIVEN self.b is not none\n",
        "  # def aggregateBranches(self):\n",
        "  #   if not self.b:\n",
        "  #     print('this node has no branches')\n",
        "  #     return\n",
        "\n",
        "  #   self.b = list(dict.fromkeys(self.b))\n",
        "  #   prompt = f'You are taking an English test.\\nHere is a question: \"{self.q}\"\\nHere is a python list of possible answers to the question: {self.b}\\nThe current python list may contain answers that are worded differently, but have the same meaning. Remove all the duplicate answers and return a python list of unique answers:'\n",
        "  #   self.b = str2lst(generateText([{'role': 'user', 'content': prompt}]))\n",
        "  #   return"
      ],
      "metadata": {
        "id": "oucXDIRuSR8j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversartion Functions"
      ],
      "metadata": {
        "id": "WpDizLttaH8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def startConv(i_want_sen = 'I want to watch a movie'):\n",
        "  prompt = stichPrompt(prompts['startConv'][0], [i_want_sen])\n",
        "  # prompt = f\"\"\"You are taking an English test.\\nHere is a sentence: \"{i_want_sen}, but I do not know [insert word]. Help me by asking me a question at a time.\"\\nReturn the sentence but with the correct word filled in the empty slot:\"\"\"\n",
        "  prompt = generateText([{'role': 'user', 'content': prompt}]).strip(\"\"\" '\" \"\"\")\n",
        "\n",
        "  context = [{'role': 'user', 'content': prompt}]\n",
        "  # appendContext(generateText([{'role': 'user', 'content': prompt}]), context, 'assistant')\n",
        "\n",
        "  return context\n",
        "\n",
        "def continueConvBool(chat_hist):\n",
        "  assert chat_hist[-1]['role'] == 'assistant', 'chat history should have the assistant as the last message'\n",
        "  prompt = stichPrompt(prompts['continueConvBool'][0], [chat_hist[-1]['content']])\n",
        "\n",
        "  return 'question' in generateText([{'role': 'user', 'content': prompt}]).lower()"
      ],
      "metadata": {
        "id": "hcDKgTw0h7Aq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convStep(context):\n",
        "  return"
      ],
      "metadata": {
        "id": "N8C7ZAhqY41N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "99lqoLANlfyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = {'startConv': [\"\"\"You are taking an English test.\\nHere is a sentence: \"{i_want_sen}, but I do not know [insert word]. Help me by asking me a question at a time.\"\\nReturn the sentence but with the correct word filled in the empty slot:\"\"\"],\n",
        "\n",
        "           'genRealProfile': [\"Generate an example profile of a possible user. The profile should be a short sequence of keywords about their personal preferences regarding {domain}.\"],\n",
        "\n",
        "           'continueConvBool': [\"Is the following text a question or a suggestion?/n/n{text}/n/nAnswer [Question, Suggestion]:\"],\n",
        "\n",
        "           'genUProfiles': ['Generate {number} different profiles. Each profile should be one sentence containing a sequence of keywords about their preferences regarding {domain}.'],\n",
        "\n",
        "           'extractLastPreference': ['Here is a conversation.\\nASSISTANT:\\n{question}\\nUSER:\\n{answer}\\nTASK: Summarize what we know about the user in a single sentence.\\nSUMMARY:'],\n",
        "\n",
        "           'genYProfiles': ['Here is a list of user preferences:{bullet_points}',\n",
        "                            'Here is a sentence describing the preferences of a user:{sentence}\\n',\n",
        "                            'Create {number} different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:',\n",
        "                            'Create different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'],\n",
        "\n",
        "           'answerQ': ['You are a character. Here is what you know about your character: {profile}\\nYou are speaking to an assistant and you speak in brief sentences.\\nAnswer the assistant in character.\\nAssistant: {question}\\nYou:'],\n",
        "\n",
        "           'pickBranch': [\"\"\"You are the following character: {profile}\\nYou are asked the following question: {question}\\n\\nFrom the options below, which is most likely your answer to the question?\\n{bullet_points}\\nRemember, you have to pick one from the list or answer \"None of the answers in the list\". Only return the most likely answer, followed by the number that is the place of the answer in the list.\\nAnswer, Number:\"\"\",\n",
        "                          \"You are the following character: {profile}\\nYou are asked the following question: {question}\\nIs it likely that you would answer: {answer}?\\n\\n[Yes, No]:\"],\n",
        "\n",
        "          'makeBranches': ['Here is a question:\\n\"{question}\"\\nReturn the smallest numbered list of different possible answers to this question. Make sure that this list, althought small, covers how any user could answer this question.\\nList of answers:'],\n",
        "\n",
        "           'makeNodes': ['Generate a python list of {node_n} questions you could possibly ask me at this point of the conversation.',\n",
        "                         'Generate a python list of questions you could possibly ask me at this point of the conversation.',\n",
        "                         ' Inlcude the last question you just asked in the python list.',\n",
        "                         \"Let's say I said the following: {answer}\\n {prompt}\"],\n",
        "\n",
        "\n",
        "           }"
      ],
      "metadata": {
        "id": "6pr0ATXMM3wJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate u profiles\n",
        "u_profiles = genUProfiles(10, 'traveling')"
      ],
      "metadata": {
        "id": "4upRYruVN7Pa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the real profile\n",
        "real_profile = genRealProfile('traveling')"
      ],
      "metadata": {
        "id": "VpFmirFtbVuF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing real profile\n",
        "pprint.pprint(real_profile.text)\n",
        "# printing their response to the following question as a quick little test\n",
        "pprint.pprint(real_profile.answerQ(real_profile, 'Are you more interested in exploration or relaxation?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdvsDF_Hczk4",
        "outputId": "b42f73c9-2d3b-4945-e669-462492b65e4b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Adventure seeker, solo traveler, budget-conscious, off-the-beaten-path, '\n",
            " 'nature lover, hiking enthusiast, cultural immersion, local experiences, '\n",
            " 'photography, backpacking, adrenaline junkie, camping, road trips, foodie, '\n",
            " 'sustainable travel, historical sites, beach lover, spontaneous, wanderlust, '\n",
            " 'wildlife encounters, authentic souvenirs')\n",
            "'Exploration.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize layer\n",
        "layer = Layer(startConv('I want to travel'))"
      ],
      "metadata": {
        "id": "TxSm2DCxKyM0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating nodes in layer, hence possible questions\n",
        "layer.makeNodes(3)"
      ],
      "metadata": {
        "id": "3YDZFlhPVul8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[n.question for n in layer.nodes]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSpmC99G0j2d",
        "outputId": "dc9f0b33-6ec4-4a14-8818-0481f6e3417a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Have you ever traveled before? If so, which destinations did you enjoy the most?',\n",
              " 'What are your interests or hobbies? Are there any specific activities or experiences you would like to try during your travels?',\n",
              " 'Do you have any budget constraints or preferences in terms of the cost of travel?']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the branches for each of the nodes (without using profiles)\n",
        "layer.makeBranches()"
      ],
      "metadata": {
        "id": "xCN97XAHPlp7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint([n.branches for n in layer.nodes])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXUb2HHiNwZl",
        "outputId": "b2f0eb14-b53f-45aa-a295-193561acc6b2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Yes, I have traveled before. My favorite destinations were Paris, Tokyo, '\n",
            "  'and New York.',\n",
            "  'Yes, I have traveled before. The destinations I enjoyed the most were the '\n",
            "  'beach destinations like Bali and Maldives.',\n",
            "  'No, I have never traveled before.'],\n",
            " ['Reading, photography, hiking, and cooking.',\n",
            "  'Playing sports, exploring different cultures, and trying out local cuisine.',\n",
            "  'Painting, attending music festivals, and learning new languages.',\n",
            "  'Yoga, scuba diving, and immersing in nature.',\n",
            "  'Camping, fishing, and visiting historical landmarks.'],\n",
            " ['Yes, I have budget constraints and prefer low-cost travel.',\n",
            "  'No, I dont have any budget constraints and am open to any cost of travel.',\n",
            "  'Yes, I have budget constraints but Im willing to consider various price '\n",
            "  'ranges for travel.',\n",
            "  'Somewhat, I have certain preferences for cost but can be flexible depending '\n",
            "  'on the travel options available.',\n",
            "  'I have specific budget limitations and can only afford travel within a '\n",
            "  'certain price range.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# layer.nodes[0].tst_b"
      ],
      "metadata": {
        "id": "5YPeTtHkP4gQ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the profiles\n",
        "layer.profiles = u_profiles\n",
        "layer.splitProfiles()"
      ],
      "metadata": {
        "id": "dF6vW0jRBWvr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in layer.nodes: print(n.profile_branch_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NVZ5BnaGUxZ",
        "outputId": "8e6a64d6-b416-4c08-df74-7a21156d1630"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 0, 1, 0, 3, 0, 3, 3, 3, 3]\n",
            "[6, 1, 3, 1, 3, 1, 3, 6, 3, 1]\n",
            "[2, 2, 2, 3, 2, 2, 2, 2, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n_idx in range(len(layer.nodes)):\n",
        "  print(f'////////// {layer.nodes[n_idx].question} //////////\\n')\n",
        "  for p_idx in range(len(layer.profiles)):\n",
        "    print(f'{layer.profiles[p_idx].text}')\n",
        "    choice = layer.nodes[n_idx].profile_branch_map[p_idx]\n",
        "    print(f'{layer.nodes[n_idx].branches[choice]}\\n')\n",
        "    # if choice is not None: print(f'{layer.nodes[n_idx].branches[choice]}\\n')\n",
        "    # else: print('None\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0hHoh7MDv_s",
        "outputId": "839ae5e3-a2c6-4433-9f54-7af6fa4d8cb5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "////////// Have you ever traveled before? If so, which destinations did you enjoy the most? //////////\n",
            "\n",
            "Adventure Seeker: Always on the lookout for adrenaline-pumping activities like skydiving, bungee jumping, and rock climbing in exotic destinations.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Culture Enthusiast: Fascinated by historical landmarks, museums, and local traditions, with a passion for exploring diverse cultures around the world.\n",
            "Yes, I have traveled before. My favorite destinations were Paris, Tokyo, and New York.\n",
            "\n",
            "Beach Lover: Craves sun-soaked shores, crystal-clear waters, and relaxation, always on the hunt for the perfect tropical paradise.\n",
            "Yes, I have traveled before. The destinations I enjoyed the most were the beach destinations like Bali and Maldives.\n",
            "\n",
            "Foodie Explorer: Driven by the desire to indulge in the finest cuisines, street food, and local delicacies, making culinary experiences a priority while traveling.\n",
            "Yes, I have traveled before. My favorite destinations were Paris, Tokyo, and New York.\n",
            "\n",
            "Nature Admirer: Drawn to picturesque landscapes, national parks, and wildlife encounters, seeking solace in the beauty of Mother Nature.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Urban Explorer: Thrives in vibrant cities, captivated by bustling streets, modern architecture, and vibrant nightlife scenes.\n",
            "Yes, I have traveled before. My favorite destinations were Paris, Tokyo, and New York.\n",
            "\n",
            "Wellness Wanderer: Seeks rejuvenation through yoga retreats, wellness spas, and mindfulness practices in serene locations.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Historical Buff: Enjoys unraveling the mysteries of ancient civilizations, exploring archaeological sites, and visiting historical monuments.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Sustainable Traveler: Committed to eco-friendly practices, passionate about responsible tourism, and supporting local communities.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Digital Nomad: Combines work and travel, seeking destinations with reliable Wi-Fi, co-working spaces, and a thriving digital nomad community.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "////////// What are your interests or hobbies? Are there any specific activities or experiences you would like to try during your travels? //////////\n",
            "\n",
            "Adventure Seeker: Always on the lookout for adrenaline-pumping activities like skydiving, bungee jumping, and rock climbing in exotic destinations.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Culture Enthusiast: Fascinated by historical landmarks, museums, and local traditions, with a passion for exploring diverse cultures around the world.\n",
            "Playing sports, exploring different cultures, and trying out local cuisine.\n",
            "\n",
            "Beach Lover: Craves sun-soaked shores, crystal-clear waters, and relaxation, always on the hunt for the perfect tropical paradise.\n",
            "Yoga, scuba diving, and immersing in nature.\n",
            "\n",
            "Foodie Explorer: Driven by the desire to indulge in the finest cuisines, street food, and local delicacies, making culinary experiences a priority while traveling.\n",
            "Playing sports, exploring different cultures, and trying out local cuisine.\n",
            "\n",
            "Nature Admirer: Drawn to picturesque landscapes, national parks, and wildlife encounters, seeking solace in the beauty of Mother Nature.\n",
            "Yoga, scuba diving, and immersing in nature.\n",
            "\n",
            "Urban Explorer: Thrives in vibrant cities, captivated by bustling streets, modern architecture, and vibrant nightlife scenes.\n",
            "Playing sports, exploring different cultures, and trying out local cuisine.\n",
            "\n",
            "Wellness Wanderer: Seeks rejuvenation through yoga retreats, wellness spas, and mindfulness practices in serene locations.\n",
            "Yoga, scuba diving, and immersing in nature.\n",
            "\n",
            "Historical Buff: Enjoys unraveling the mysteries of ancient civilizations, exploring archaeological sites, and visiting historical monuments.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Sustainable Traveler: Committed to eco-friendly practices, passionate about responsible tourism, and supporting local communities.\n",
            "Yoga, scuba diving, and immersing in nature.\n",
            "\n",
            "Digital Nomad: Combines work and travel, seeking destinations with reliable Wi-Fi, co-working spaces, and a thriving digital nomad community.\n",
            "Playing sports, exploring different cultures, and trying out local cuisine.\n",
            "\n",
            "////////// Do you have any budget constraints or preferences in terms of the cost of travel? //////////\n",
            "\n",
            "Adventure Seeker: Always on the lookout for adrenaline-pumping activities like skydiving, bungee jumping, and rock climbing in exotic destinations.\n",
            "Yes, I have budget constraints but Im willing to consider various price ranges for travel.\n",
            "\n",
            "Culture Enthusiast: Fascinated by historical landmarks, museums, and local traditions, with a passion for exploring diverse cultures around the world.\n",
            "Yes, I have budget constraints but Im willing to consider various price ranges for travel.\n",
            "\n",
            "Beach Lover: Craves sun-soaked shores, crystal-clear waters, and relaxation, always on the hunt for the perfect tropical paradise.\n",
            "Yes, I have budget constraints but Im willing to consider various price ranges for travel.\n",
            "\n",
            "Foodie Explorer: Driven by the desire to indulge in the finest cuisines, street food, and local delicacies, making culinary experiences a priority while traveling.\n",
            "Somewhat, I have certain preferences for cost but can be flexible depending on the travel options available.\n",
            "\n",
            "Nature Admirer: Drawn to picturesque landscapes, national parks, and wildlife encounters, seeking solace in the beauty of Mother Nature.\n",
            "Yes, I have budget constraints but Im willing to consider various price ranges for travel.\n",
            "\n",
            "Urban Explorer: Thrives in vibrant cities, captivated by bustling streets, modern architecture, and vibrant nightlife scenes.\n",
            "Yes, I have budget constraints but Im willing to consider various price ranges for travel.\n",
            "\n",
            "Wellness Wanderer: Seeks rejuvenation through yoga retreats, wellness spas, and mindfulness practices in serene locations.\n",
            "Yes, I have budget constraints but Im willing to consider various price ranges for travel.\n",
            "\n",
            "Historical Buff: Enjoys unraveling the mysteries of ancient civilizations, exploring archaeological sites, and visiting historical monuments.\n",
            "Yes, I have budget constraints but Im willing to consider various price ranges for travel.\n",
            "\n",
            "Sustainable Traveler: Committed to eco-friendly practices, passionate about responsible tourism, and supporting local communities.\n",
            "Yes, I have budget constraints but Im willing to consider various price ranges for travel.\n",
            "\n",
            "Digital Nomad: Combines work and travel, seeking destinations with reliable Wi-Fi, co-working spaces, and a thriving digital nomad community.\n",
            "Yes, I have budget constraints but Im willing to consider various price ranges for travel.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer.calcInfoGain()"
      ],
      "metadata": {
        "id": "W4RQSuQNERLM"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in layer.nodes:\n",
        "  print(n.info_gains)\n",
        "\n",
        "print(layer.nodes[layer.best_node_idx].question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6EZ6QCsk4Lp",
        "outputId": "26b8be10-353f-42a4-cf69-825a926088ca"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.295461844238322\n",
            "1.5219280948873621\n",
            "0.4689955935892809\n",
            "What are your interests or hobbies? Are there any specific activities or experiences you would like to try during your travels?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer.context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hra2eNaGXebO",
        "outputId": "8516c417-f57f-4bc1-80d7-b79150e4a8cd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user',\n",
              "  'content': 'I want to travel, but I do not know where. Help me by asking me a question at a time.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OLD CODE"
      ],
      "metadata": {
        "id": "lsb2udidn1NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_with_idk(options, p_idk = 0.2):\n",
        "  return random.choices([random.choice(options), \"I don't know\"], weights = [1-p_idk, p_idk], k=1)[0]"
      ],
      "metadata": {
        "id": "SOdIZFFGEca2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_branch(q, branches, profiles):\n",
        "  ans = []\n",
        "\n",
        "  for p in profiles:\n",
        "    prompt = f\"\"\"You are the following character: {p}\\nYou are asked the following question: {q}\\n\\nFrom the options below, which is most likely your answer to the question?\\n{list2numbered(branches)}\\nRemember, you have to pick one from the list. Only return the most likely answer, followed by the number that is the place of the answer in the list.\\nAnswer, Number:\"\"\"\n",
        "    txt = generateText([{'role': 'user', 'content': prompt}])\n",
        "    m = re.search('\\d', txt)\n",
        "    if m:\n",
        "      s = m.start()\n",
        "      e = m.end()\n",
        "      ans.append(int(txt[s:e]))\n",
        "    else: ans.append(0)\n",
        "\n",
        "\n",
        "  return ans"
      ],
      "metadata": {
        "id": "Bkkg1IiyELh9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genAnswP(profile, q):\n",
        "  prompt = f'You are a character. Here is what you know about your character: {profile}\\nYou are speaking to an assistant and you speak in brief sentences.\\nAnswer the assistant in character.\\nAssistant: {q}\\nYou:'\n",
        "  return generateText([{'role': 'user', 'content': prompt}])\n",
        "\n",
        "def genAns(q):\n",
        "  prompt = f'Here is a question:\\n\"{q}\"\\nReturn a python list with all the possible answers to the question:'\n",
        "  txt = generateText([{'role': 'user', 'content': prompt}], tries = 2)\n",
        "  return str2lst(txt)\n",
        "\n",
        "def aggregateAns(ans_lst, q = None):\n",
        "  print('aggregating')\n",
        "  ans_lst = list(dict.fromkeys(ans_lst))\n",
        "  #prompt = f'Here is a list of preferences: {ans_lst}\\nRemove all the answers that mean the same thing and return a list of unique answers:' BEST\n",
        "  # prompt = f'Here is a list of preferences: {ans_lst}\\nRemove all the answers that are too similar and return a small list of unique answers:'\n",
        "  if q:\n",
        "    #prompt = f'You will be given a python list that has possible answers to the question \"{q}\"\\nYour task is to remove all the answers that mean the same thing and return a small list of unique answers.\\nHere is the python list: {ans_lst}'\n",
        "    prompt = f'You are taking an English test.\\nHere is a question: \"{q}\"\\nHere is a python list of possible answers to the question: {ans_lst}\\nThe current python list contains answers that are worded differently, but contain the same meaning. Remove all the duplicate answers and return a python list of unique answers:'\n",
        "  else:\n",
        "    prompt = f'Here is a list of preferences: {ans_lst}\\nRemove all the answers that mean the same thing and return a list of unique answers:'\n",
        "\n",
        "  return str2lst(generateText([{'role': 'user', 'content': prompt}]))"
      ],
      "metadata": {
        "id": "ZPAfvOyd_sgv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class representing a group of profiles\n",
        "class ProfileGroup:\n",
        "  def __init__(self, preference_lst = None):\n",
        "    self.preference_lst = preference_lst\n",
        "    self.profiles = []\n",
        "    # if preference_lst:\n",
        "    #   self.profiles = self.makeYProfiles()\n",
        "    #   self.pType = ProfileType.Y\n",
        "    # else:\n",
        "    #   self.profiles = self.makeUProfiles()\n",
        "    #   self.pType = ProfileType.U\n",
        "    return\n",
        "\n",
        "  # each profile answers question q, returns list of strings\n",
        "  def answerQ(self, q):\n",
        "    return multiThread(Profile.answerQ, [[p, self.q] for p in self.profiles])\n",
        "\n",
        "  # each profile picks branch that is best fit, returns list of int\n",
        "  def pickBranch(self, node):\n",
        "    return multiThread(Profile.pickBranch, [[p, node] for p in self.profiles])\n",
        "\n",
        "  def makeUProfiles(self, prof_num):\n",
        "    prompt = f'Generate {prof_num} different profiles. Each profile should be one sentence containing a sequence of keywords about their preferences regarding traveling.'\n",
        "    txt = generateText([{'role': 'user', 'content': prompt}], tries = 2)\n",
        "    self.profiles.extend(str2lst(txt))\n",
        "    return\n",
        "\n",
        "  def makeYProfiles(self):\n",
        "    p_n = len(self.preference_lst)\n",
        "    prompt = self.preference_lst[0]\n",
        "\n",
        "    # if preference_lst has more than one preferece, create a single sentence representing all of them\n",
        "    if p_n > 1:\n",
        "      prompt = f'Here is a list of user preferences:{list2numbered(self.preference_lst)}'\n",
        "      prompt = generateText([{'role': 'user', 'content': prompt+'Create a single sentence of key words that describes all of these preferences:'}], tries = 2)\n",
        "\n",
        "    prompt = f'Here is a sentence describing the preferences of a user:{prompt}\\n'\n",
        "\n",
        "    # if profile number is specified, generate this specific number, otherwise let LLM decide\n",
        "    prompt += f'Create {self.prof_num} different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'\n",
        "    # prompt += 'Create different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'\n",
        "\n",
        "    return str2lst(generateText([{'role': 'user', 'content': prompt}], tries = 2))"
      ],
      "metadata": {
        "id": "Dh8P2VoV0XRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs_ans = [[] for q in range(len(p_qs))]\n",
        "for idx, a in enumerate(ans): qs_ans[idx%len(p_qs)].append(a)\n",
        "print(len(qs_ans))\n",
        "print(len(qs_ans[0]))"
      ],
      "metadata": {
        "id": "ufNSNJfmjonl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs_ans[0]"
      ],
      "metadata": {
        "id": "phATC3i17x24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aggregating answers\n",
        "branches = [[] for q in range(len(p_qs))]\n",
        "for i, ans in enumerate(qs_ans):\n",
        "  branches[i] = aggregateAns(ans)\n",
        "\n",
        "for b in branches:\n",
        "  print(len(b))\n",
        "pprint.pprint(branches)"
      ],
      "metadata": {
        "id": "JXc9psnpvNIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "branches = [[] for q in range(len(p_qs))]\n",
        "for i, ans in enumerate(qs_ans):\n",
        "  branches[i] = aggregateAns(ans, p_qs[i])\n",
        "\n",
        "for b in branches:\n",
        "  print(len(b))\n",
        "pprint.pprint(branches)"
      ],
      "metadata": {
        "id": "LDrBGPj8MRCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}