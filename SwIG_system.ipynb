{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZpwBpJVs2WV8",
        "ojlPwz7TGvxd",
        "x7CeFKIQ3yIL",
        "lsb2udidn1NM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Possible Paper Titles\n",
        "\n",
        "1) Improving LLM conversational recommendations: How Empathy Leads to Information Gain\n",
        "\n",
        "2) SwIG: Sampling with Information Gain to improve conversational recommendations"
      ],
      "metadata": {
        "id": "CnLFBHTP1kea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to Drive and Loading Requirements\n"
      ],
      "metadata": {
        "id": "ZpwBpJVs2WV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4uSeYoA1qaY",
        "outputId": "1f77be0f-a346-446a-8e81-c897033a17b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_dir = '/content/drive/MyDrive/Clarifying_Questions_GPT_Research'"
      ],
      "metadata": {
        "id": "dXxVUm8D150e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd {drive_dir} && pip install -r requirements_2.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34bZLvmV17Lk",
        "outputId": "4aa3e696-3cb8-4a7a-deab-cba881c98b67"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 2)) (0.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 3)) (1.23.5)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 4)) (0.28.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 5)) (1.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 6)) (9.4.0)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 7)) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 8)) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 9)) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements_2.txt (line 10)) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements_2.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from matplotlib-inline->-r requirements_2.txt (line 2)) (5.7.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements_2.txt (line 4)) (3.8.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements_2.txt (line 5)) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements_2.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements_2.txt (line 8)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements_2.txt (line 8)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements_2.txt (line 8)) (2023.7.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements_2.txt (line 9)) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements_2.txt (line 9)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements_2.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements_2.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements_2.txt (line 4)) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "import pickle\n",
        "\n",
        "import json\n",
        "\n",
        "import threading\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "\n",
        "import re\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import math\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import copy\n",
        "\n",
        "import signal\n",
        "\n",
        "import time\n",
        "\n",
        "import sys\n",
        "\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pprint\n",
        "\n",
        "import urllib3"
      ],
      "metadata": {
        "id": "DQObQKqP3fn_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(drive_dir+\"/openai_key.key\", \"r\") as f:\n",
        "    openai.api_key = f.read()"
      ],
      "metadata": {
        "id": "UZOZNb1tnmDs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Functions and Decorators"
      ],
      "metadata": {
        "id": "ojlPwz7TGvxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# given a string that has a list, creates a python list with the input's contents\n",
        "def str2lst(l):\n",
        "\n",
        "  # if it is a pyhton list as a str\n",
        "  if '[' in l and ']' in l:\n",
        "    # removing double spaces and newlines\n",
        "    l = re.sub(\"\\s+\", \" \", l)\n",
        "    # removing anything that isn't the list\n",
        "    m = re.search('\\[(.|\\s)+\\]', l)\n",
        "    l = l[m.start():m.end()]\n",
        "    # converting string to list\n",
        "    l = l.strip('][')\n",
        "    l = re.split(\"\"\"['\"], \"\"\", l)\n",
        "\n",
        "    # spaghetti code to dealing with this weird bug I saw before\n",
        "    if len(l) == 1:\n",
        "      l = l[0]\n",
        "      l = re.sub(\"-\", \"\", l)\n",
        "      l = l.split('\\n')\n",
        "\n",
        "  # else if it is bullet points tha are...\n",
        "  #numbered\n",
        "  elif re.search('\\d\\.', l):\n",
        "    # converting string to list\n",
        "    l = re.findall('\\d\\.\\s(.+)', l)\n",
        "\n",
        "  #not numbered\n",
        "  else:\n",
        "    # converting string to list\n",
        "    l = re.split('-', l)[1:]\n",
        "\n",
        "  # removing extra quotation marks\n",
        "  for i in range(len(l)): l[i] = re.sub(\"\"\"['\"]\"\"\", \"\", l[i])\n",
        "  # removing trailing and starting spaces\n",
        "  for i in range(len(l)): l[i] = l[i].strip(' ')\n",
        "\n",
        "  return l\n",
        "\n",
        "# function to turn a python list into a numbered list as a string\n",
        "def list2numbered(lst):\n",
        "  num_lst = []\n",
        "  for idx in range(len(lst)): num_lst.append(f'{idx+1}. {lst[idx]}\\n')\n",
        "  return ''.join(num_lst)\n",
        "\n",
        "# decorator that saves the output of func into a specific index of a lst (for multi threading)\n",
        "def saveInLst(lst, idx):\n",
        "\n",
        "  def inner(func):\n",
        "\n",
        "    def wrap(*args, **kwargs):\n",
        "      lst[idx] = func(*args, **kwargs)\n",
        "      return\n",
        "\n",
        "    return wrap\n",
        "\n",
        "  return inner\n",
        "\n",
        "# given a function, a list of input lists, creates a thread for each input list, running the function with different inouts in parallel. Returns a list of outputs\n",
        "def multiThread(func, input_lst):\n",
        "  thread_n = len(input_lst)\n",
        "  threads = []\n",
        "\n",
        "  results = [None for i in range(thread_n)]\n",
        "\n",
        "  n = 80\n",
        "  idxs = [i for i in range(thread_n )]\n",
        "  batch_idxs = [idxs[i * n:(i + 1) * n] for i in range((len(idxs) + n - 1) // n )]\n",
        "  # print(batch_idxs)\n",
        "  for b in batch_idxs:\n",
        "    # print(b)\n",
        "    for i in b:\n",
        "      time.sleep(2) # my attempt to stop hitting the rate limit\n",
        "      tr = threading.Thread(target=saveInLst(results, i)(func), args=(*input_lst[i],))\n",
        "      # tr = threading.Thread(target=func, args=(*input_lst[i],))\n",
        "      tr.start()\n",
        "      threads.append(tr)\n",
        "\n",
        "    for tr in threads:\n",
        "      tr.join()\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "1PYfBrn-Gwh0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for Text Generation"
      ],
      "metadata": {
        "id": "x7CeFKIQ3yIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generates text using gpt-3.5 given a conversation context, returns None if its not able to generate text after a certain number of tries\n",
        "def generateText(context, temp = 0.7, tries = 3, max_time = 120):\n",
        "  assert isinstance(context, list), f'input is not a context list, got {type(context)}'\n",
        "\n",
        "  resp = None\n",
        "\n",
        "  while tries != 0:\n",
        "    try:\n",
        "      resp = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=context,\n",
        "        temperature=temp)['choices'][0]['message']['content']\n",
        "\n",
        "      tries = 0\n",
        "    except:\n",
        "      tries -= 1\n",
        "\n",
        "  return resp\n",
        "\n",
        "\n",
        "# def generateMText(context, thread_n, temp = 0.7, tries = 3):\n",
        "#   threads = []\n",
        "\n",
        "#   results = [None for i in range(thread_n)]\n",
        "\n",
        "#   seed = 'Generate a profile of a user who is looking to travel. The profile should be comprised of a few bullet points that describe them'\n",
        "\n",
        "#   for i in range(thread_n):\n",
        "#     tr = threading.Thread(target=saveInLst(results, i)(generateText), args=(context, temp, tries,))\n",
        "#     tr.start()\n",
        "#     threads.append(tr)\n",
        "\n",
        "#   for tr in threads:\n",
        "#     tr.join()\n",
        "\n",
        "#   return results\n",
        "\n",
        "# appends a new entry to a conversation context\n",
        "def appendContext(text, context, role = 'user'):\n",
        "  assert role == 'user' or role == 'assistant', f'unexpected role, got: {role}'\n",
        "\n",
        "  if not context:\n",
        "    context.append({'role': role, 'content': text})\n",
        "    return\n",
        "\n",
        "  if role == 'user':\n",
        "    assert context[-1]['role'] == 'assistant', f'incompatible adjecent role, got user'\n",
        "  else:\n",
        "    assert context[-1]['role'] == 'user', f'incompatible adjecent role, got assistant'\n",
        "\n",
        "  context.append({'role': role, 'content': text})\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "LtqU76cG3xZe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Everything Else"
      ],
      "metadata": {
        "id": "PR03-I18siAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts"
      ],
      "metadata": {
        "id": "tlvTRnk4ZqXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stichPrompt(prompt, var_lst):\n",
        "  var_places = len(re.findall('{\\S*}', prompt))\n",
        "  assert var_places == len(var_lst), f'prompt has to have the same number of places for variables as variables in var_lst. Prompt has {var_places} var places and len(var_lst) = {len(var_lst)}. Here is the prompt:\\n{prompt}'\n",
        "  for var_idx in range(var_places):\n",
        "    m = re.search('{\\S*}', prompt)\n",
        "    start = prompt[:m.start()]\n",
        "    end = prompt[m.end():]\n",
        "    prompt = f'{start}{var_lst[var_idx]}{end}'\n",
        "\n",
        "  return prompt\n",
        "\n",
        "# CREATE CODE THAT SERVES AS DICT OF ALL PROPMTS (WHERE THE VALUES AZRE FUNCTIONS)\n",
        "stichPrompt(\"Here is an example prompt, with a var here: {var} and here: {}\", [1,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OiVngHxyJd3L",
        "outputId": "f9aedebf-7fa3-43c9-fcb2-25afc33e4c62"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is an example prompt, with a var here: 1 and here: 2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = {'startConv': [\"\"\"You are taking an English test.\\nHere is a sentence: \"{i_want_sen}, but I do not know [insert word]. Help me by asking me a question at a time.\"\\nReturn the sentence but with the correct word filled in the empty slot:\"\"\"],\n",
        "\n",
        "           'genRealProfile': [\"Generate a profile of a user. The profile should be a list of keywords about their preferences regarding {domain}.\"],\n",
        "\n",
        "           'continueConvBool': [\"Is the following text a question or a suggestion?/n/n{text}/n/nAnswer [Question, Suggestion]:\"],\n",
        "\n",
        "           'genUProfiles': ['Generate {number} different profiles. Each profile should be one sentence containing a sequence of keywords about their preferences regarding {domain}.'],\n",
        "\n",
        "           'extractLastPreference': ['Here is a conversation.\\nASSISTANT:\\n{question}\\nUSER:\\n{answer}\\nTASK: Summarize what we know about the user in a single sentence.\\nSUMMARY:'],\n",
        "\n",
        "           'genYProfiles': ['Here is a list of user preferences:{bullet_points}',\n",
        "                            'Here is a sentence describing the preferences of a user:{sentence}\\n',\n",
        "                            'Create {number} different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:',\n",
        "                            'Create different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'],\n",
        "\n",
        "           'answerQ': ['You are a character. Here is what you know about your character: {profile}\\nYou are speaking to an assistant and you speak in brief sentences.\\nAnswer the assistant in character.\\nAssistant: {question}\\nYou:'],\n",
        "\n",
        "           'pickBranch': [\"\"\"You are the following character: {profile}\\nYou are asked the following question: {question}\\n\\nFrom the options below, which is most likely your answer to the question?\\n{bullet_points}\\nRemember, you have to pick one from the list or answer \"None of the answers in the list\". Only return the most likely answer, followed by the number that is the place of the answer in the list.\\nAnswer, Number:\"\"\",\n",
        "                          \"You are the following character: {profile}\\nYou are asked the following question: {question}\\nIs it likely that you would answer: {answer}?\\n\\n[Yes, No]:\"],\n",
        "\n",
        "          'makeBranches': ['Here is a question:\\n\"{question}\"\\nReturn the smallest numbered list of different possible answers to this question. Make sure that this list, althought small, covers how any user could answer this question.'],\n",
        "\n",
        "           'makeNodes': ['Generate a python list of {node_n} questions you could possibly ask me at this point of the conversation.',\n",
        "                         'Generate a python list of questions you could possibly ask me at this point of the conversation.',\n",
        "                         ' Inlcude the last question you just asked in the python list.',\n",
        "                         \"Let's say I said the following: {answer}\\n {prompt}\"],\n",
        "\n",
        "\n",
        "           }"
      ],
      "metadata": {
        "id": "YJdxUfJ7CAeq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for Profiles"
      ],
      "metadata": {
        "id": "UTSV5_KrZxY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create U Profiles\n",
        "def genUProfiles(prof_num = 50, domain = 'movies'):\n",
        "  prompt = stichPrompt(prompts['genUProfiles'][0], [prof_num, domain])\n",
        "  #prompt = f'Generate {prof_num} different profiles. Each profile should be one sentence containing a sequence of keywords about their preferences regarding traveling.'\n",
        "  return [Profile(txt, ProfileType.U) for txt in str2lst(generateText([{'role': 'user', 'content': prompt}], tries = 2))]\n",
        "\n",
        "# Extract the last preference the user revealed in the chat context\n",
        "def extractLastPreference(chat_hist):\n",
        "  chat_hist = chat_hist[-2:]\n",
        "  assert chat_hist[0]['role'] == 'assistant', 'chat history should have the assistant as the second to last message'\n",
        "  assert chat_hist[1]['role'] == 'user', 'chat history should have the user as the last message'\n",
        "\n",
        "  prompt = stichPrompt(prompts['extractLastPreference'][0], [chat_hist[0]['content'], chat_hist[1]['content']])\n",
        "  # prompt = f'Here is a conversation.\\nASSISTANT:\\n{chat_hist[0]['content']}\\nUSER:\\n{chat_hist[1]['content']}\\nTASK: Summarize what we know about the user in a single sentence.\\nSUMMARY:'\n",
        "\n",
        "  return generateText([{'role': 'user', 'content': prompt}], tries = 2)\n",
        "\n",
        "# Create Y Profiles\n",
        "def genYProfiles(preference_lst = [], prof_num = None):\n",
        "  p_n = len(preference_lst)\n",
        "  assert p_n != 0, 'need at least one preference in preference_lst'\n",
        "\n",
        "  prompt = preference_lst[0]\n",
        "\n",
        "  # if preference_lst has more than oone preferece, create a single sentence representing all of them\n",
        "  if p_n > 1:\n",
        "    prompt = stichPrompt(prompts['genYProfiles'][0], [list2numbered(preference_lst)])\n",
        "    # prompt = f'Here is a list of user preferences:{list2numbered(preference_lst)}'\n",
        "    prompt = generateText([{'role': 'user', 'content': prompt+'Create a single sentence of key words that describes all of these preferences:'}], tries = 2)\n",
        "\n",
        "  prompt = stichPrompt(prompts['genYProfiles'][1], [prompt])\n",
        "  # prompt = f'Here is a sentence describing the preferences of a user:{prompt}\\n'\n",
        "\n",
        "  # if profile number is specified, generate this specific number, otherwise let LLM decide\n",
        "  if prof_num:\n",
        "    prompt = stichPrompt(prompts['genYProfiles'][2], [prof_num])\n",
        "    # prompt += f'Create {prof_num} different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'\n",
        "  else:\n",
        "    prompt += prompts['genYProfiles'][3]\n",
        "    # prompt += 'Create different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'\n",
        "\n",
        "  return [Profile(txt, ProfileType.Y) for txt in str2lst(generateText([{'role': 'user', 'content': prompt}], tries = 2))]\n",
        "\n",
        "# Create a real profile\n",
        "def genRealProfile(domain = 'movies'):\n",
        "  prompt = stichPrompt(prompts['genRealProfile'][0], [domain])\n",
        "  #prompt = f'Generate {prof_num} different profiles. Each profile should be one sentence containing a sequence of keywords about their preferences regarding traveling.'\n",
        "  return Profile(generateText([{'role': 'user', 'content': prompt}], tries = 2), ProfileType.R)"
      ],
      "metadata": {
        "id": "7Y78U4RxHbDB"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree Structure"
      ],
      "metadata": {
        "id": "4haEyKYJZ53U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enum of Profile types\n",
        "class ProfileType(Enum):\n",
        "  U = 0\n",
        "  Y = 1\n",
        "  R = 2\n",
        "\n",
        "# Class representing a user profile\n",
        "class Profile:\n",
        "  def __init__(self, text: str, p_type: ProfileType):\n",
        "    self.text = text\n",
        "    self.p_type = p_type\n",
        "    return\n",
        "\n",
        "  # # makes this Proifle into a UProfile\n",
        "  # def makeUProfile(self):\n",
        "  #   self.p_type = ProfileType.U\n",
        "  #   return\n",
        "\n",
        "  # # makes this Proifle into a YProfile\n",
        "  # def makeYProfile(self, context):\n",
        "  #   self.p_type = ProfileType.Y\n",
        "  #   return\n",
        "\n",
        "  # answers question q as the user the profile describes, returns string\n",
        "  @staticmethod\n",
        "  def answerQ(self, q):\n",
        "    prompt = stichPrompt(prompts['answerQ'][0], [self.text, q])\n",
        "    # prompt = f'You are a character. Here is what you know about your character: {self.text}\\nYou are speaking to an assistant and you speak in brief sentences.\\nAnswer the assistant in character.\\nAssistant: {q}\\nYou:'\n",
        "    return generateText([{'role': 'user', 'content': prompt}])\n",
        "\n",
        "  # picks branch that best fits profile, returns int that is index of picked branch in the branches list or None\n",
        "  @staticmethod\n",
        "  def pickBranch(self, node):\n",
        "    prompt = stichPrompt(prompts['pickBranch'][0], [self.text, node.question, list2numbered(node.branches)])\n",
        "    # prompt = f\"\"\"You are the following character: {self.text}\\nYou are asked the following question: {node.question}\\n\\nFrom the options below, which is most likely your answer to the question?\\n{list2numbered(node.branches)}\\nRemember, you have to pick one from the list or answer \"None of the answers in the list\". Only return the most likely answer, followed by the number that is the place of the answer in the list.\\nAnswer, Number:\"\"\"\n",
        "    txt = generateText([{'role': 'user', 'content': prompt}], temp=1.0)\n",
        "\n",
        "    # trying to find a number in the answer generated\n",
        "    n_m = re.search('\\d', txt)\n",
        "\n",
        "    if n_m:\n",
        "      # if the number chosen is not an index to the branches list, return None\n",
        "      branch_idx = int(txt[n_m.start():n_m.end()]) - 1\n",
        "      if branch_idx >= len(node.branches):\n",
        "        return\n",
        "\n",
        "      # if given an appropriate answer, make sure the answer shouldn't be neither\n",
        "      prompt = stichPrompt(prompts['pickBranch'][1], [self.text, node.question, node.branches[branch_idx]])\n",
        "      # prompt = f\"You are the following character: {self.text}\\nYou are asked the following question: {node.question}\\nIs it likely that you would answer: {txt[t_m.start():t_m.end()]}?\\n\\n[Yes, No]:\"\n",
        "\n",
        "      if 'yes' in generateText([{'role': 'user', 'content': prompt}]).lower():\n",
        "        return branch_idx\n",
        "\n",
        "    # if no number was given, or if the number was not an appropriate answer, return None\n",
        "    return\n",
        "\n",
        "# Class representing a node in a layer of the \"decision tree\"\n",
        "class Node:\n",
        "  def __init__(self, q : str):\n",
        "    # question that represents this node\n",
        "    self.question = q\n",
        "    # possible branches/answers to this node/question\n",
        "    self.branches = None\n",
        "    self.tst_b = None\n",
        "    # map of profile index (from given profile_lst input in splitProfiles func) to branch index\n",
        "    self.profile_branch_map = None\n",
        "    # entropy and info gain for this Node\n",
        "    self.entropy = None\n",
        "    self.info_gains = None\n",
        "\n",
        "    return\n",
        "\n",
        "  # generates branches for this node\n",
        "  @staticmethod\n",
        "  def makeBranches(self):\n",
        "    assert self.question != None, \"self.question cannot be None\"\n",
        "\n",
        "    prompt = stichPrompt(prompts['makeBranches'][0], [self.question])\n",
        "    # prompt = f'Here is a question:\\n\"{self.question}\"\\nReturn a concise list of different possible answers to the question.'\n",
        "    self.tst_b = generateText([{'role': 'user', 'content': prompt}], tries = 2, temp = 1.0)\n",
        "    self.branches = str2lst(self.tst_b)\n",
        "    return\n",
        "\n",
        "  # splits profiles into the different branches, then creates a mapping from profile index to branch index\n",
        "  @staticmethod\n",
        "  def splitProfiles(self, profile_lst : List[Profile]):\n",
        "    assert self.branches != None, \"self.branches cannot be None\"\n",
        "    assert len(profile_lst) != 0, \"self.profile_lst cannot be empty\"\n",
        "\n",
        "    # each profile picks branch that is best fit, returns list of int\n",
        "    # saving what profile picked what branch in self.profile_branch_map\n",
        "    self.profile_branch_map = multiThread(Profile.pickBranch, [[p, self] for p in profile_lst])\n",
        "\n",
        "    # editing self.branches and self.profile_branch_map if a profile picked no Branches\n",
        "    has_none = False\n",
        "    for idx in range(len(self.profile_branch_map)):\n",
        "      if self.profile_branch_map[idx] == None:\n",
        "        self.profile_branch_map[idx] = len(self.branches)\n",
        "        has_none = True\n",
        "\n",
        "    if has_none:\n",
        "      self.branches.append(\"NONE OF THE ABOVE\")\n",
        "\n",
        "    return\n",
        "\n",
        "  # calculating info gain\n",
        "  def clalcInfoGain(self, profile_lst : List[Profile]):\n",
        "    # number of total profiles\n",
        "    total_profiles_n = len(profile_lst)\n",
        "    # creating a set of classes\n",
        "    classes = set()\n",
        "    for p in profile_lst:\n",
        "      classes.add(p.p_type)\n",
        "    # map from branch index to count of profiles that picked this branch\n",
        "    branch_count = Counter(self.profile_branch_map)\n",
        "    # map from branch index to possible node entropy\n",
        "    branch_entropy = []\n",
        "\n",
        "    # if there is only one class, treat every profile as its own class\n",
        "    if len(classes) == 1:\n",
        "      # calculating this node's entropy\n",
        "      self.entropy = -1*math.log2(1/total_profiles_n)\n",
        "\n",
        "      # calculating the entropy of each branch\n",
        "      for b_idx in range(len(self.branches)):\n",
        "        if branch_count[b_idx] != 0: branch_entropy.append(-1*math.log2(1/branch_count[b_idx]))\n",
        "        else: branch_entropy.append(0)\n",
        "\n",
        "    # otherwise\n",
        "    else:\n",
        "      # calculating this node's entropy\n",
        "      self.entropy = 0\n",
        "      for c in Counter([p.p_type.value for p in profile_lst]).values(): self.entropy -= (c/total_profiles_n)*math.log2(c/total_profiles_n)\n",
        "\n",
        "      # calculating the entropy of each branch\n",
        "      assert 1==0\n",
        "\n",
        "    # calculating information gain\n",
        "    branch_entropy_weighted_sum = 0\n",
        "    for b_idx in range(len(self.branches)):\n",
        "      branch_entropy_weighted_sum += (branch_count[b_idx]/total_profiles_n) * branch_entropy[b_idx]\n",
        "\n",
        "    self.info_gains = self.entropy - branch_entropy_weighted_sum\n",
        "    return\n",
        "\n",
        "\n",
        "# Class representing a layer of the \"decision tree\"\n",
        "class Layer:\n",
        "  def __init__(self, context):\n",
        "    # current context\n",
        "    self.context = context\n",
        "    # list of nodes, each representing a possible question for this layer of the conversation\n",
        "    self.nodes = None\n",
        "\n",
        "    # list of Profile\n",
        "    self.profiles = []\n",
        "\n",
        "    # index of the node with the highest information gain\n",
        "    self.best_node_idx = None\n",
        "\n",
        "    # stores next layer\n",
        "    self.next = None\n",
        "    # stores previous layer\n",
        "    self.prev = None\n",
        "\n",
        "    return\n",
        "\n",
        "  # creating U Profiles\n",
        "  def makeUProfiles(self):\n",
        "    return\n",
        "\n",
        "  # creating nodes, hence possible questions for this layer\n",
        "  def makeNodes(self, node_n = None):\n",
        "    # making a deep copy of the context so it is unchanged\n",
        "    context = copy.deepcopy(self.context)\n",
        "\n",
        "    # making the base prompt\n",
        "    if node_n:\n",
        "      prompt = stichPrompt(prompts['makeNodes'][0], [node_n])\n",
        "      # prompt = f'Generate a python list of {node_n} questions you could possibly ask me at this point of the conversation.'\n",
        "    else:\n",
        "      prompt = prompts['makeNodes'][1]\n",
        "      # prompt = 'Generate a python list of questions you could possibly ask me at this point of the conversation.'\n",
        "\n",
        "    if context[-1]['role'] == 'assistant':\n",
        "      prompt += prompts['makeNodes'][2]\n",
        "      # prompt += ' Inlcude the last question you just asked in the python list.'\n",
        "    else:\n",
        "      prompt = stichPrompt(prompts['makeNodes'][3], [context.pop()['content'], prompt])\n",
        "      # prompt = f\"Let's say I said the following: {context.pop()['content']}\\n {prompt}\"\n",
        "\n",
        "\n",
        "    # appending the prompt to context\n",
        "    appendContext(prompt, context, 'user')\n",
        "\n",
        "    # generating the questions and storing them as a python list, then initializing nodes\n",
        "    self.nodes = [Node(q) for q in str2lst(generateText(context, tries = 2))]\n",
        "    return\n",
        "\n",
        "  # creates branches for each Node in self.nodes\n",
        "  def makeBranches(self):\n",
        "    assert self.nodes != None, \"self.nodes cannot be None\"\n",
        "    multiThread(Node.makeBranches, [[n] for n in self.nodes])\n",
        "    return\n",
        "\n",
        "  # create a mapping from profiles to branches for each node\n",
        "  def splitProfiles(self):\n",
        "    assert self.nodes != None, \"self.nodes cannot be None\"\n",
        "    assert len(self.profiles) != 0, \"self.profiles cannot be empty\"\n",
        "    multiThread(Node.splitProfiles, [[n, self.profiles] for n in self.nodes])\n",
        "    return\n",
        "    # for n in self.nodes:\n",
        "    #   n.splitProfiles(self.profiles)\n",
        "    # return\n",
        "\n",
        "  # calculates info gain for every node and saves index of best node\n",
        "  def calcInfoGain(self):\n",
        "    for n in self.nodes: n.clalcInfoGain(self.profiles)\n",
        "    ig = [n.info_gains for n in self.nodes]\n",
        "    self.best_node_idx = ig.index(max(ig))\n",
        "\n",
        "    return\n",
        "\n",
        "  # ## NEED TO IMPLEMENT THIS FOR NEW CONCENPTION OF NODE (AS WELL AS ALL THE OTHER FUNCTIONS)\n",
        "  # # creates branches using profiles by calling the ProfileGroup pickbranch(self, node) method for the ProfileGroup in index profile_group_idx in self.profile_groups\n",
        "  # def makeBranchesP(self, profile_group_idx = None):\n",
        "  #   if profile_group_idx and len(self.profiles) <= profile_group_idx:\n",
        "  #     print(f'self.profiles does not have index {profile_group_idx}')\n",
        "  #     return\n",
        "\n",
        "  #   if profile_group_idx and len(self.profiles[profile_group_idx]) == 0:\n",
        "  #     print(f'no profiles exists in index {profile_group_idx }')\n",
        "  #     return\n",
        "\n",
        "  #   if profile_group_idx:\n",
        "  #     b_maker_ps = self.profiles[profile_group_idx]\n",
        "  #   else:\n",
        "  #     b_maker_ps = [p for p_group in self.profiles for p in p_group]\n",
        "\n",
        "  #   self.b = multiThread(Profile.answerQ, [[p, self.q] for p in b_maker_ps])\n",
        "  #   return\n",
        "\n",
        "  # # aggregate branches that are the same, sets self.b to a list of strings GIVEN self.b is not none\n",
        "  # def aggregateBranches(self):\n",
        "  #   if not self.b:\n",
        "  #     print('this node has no branches')\n",
        "  #     return\n",
        "\n",
        "  #   self.b = list(dict.fromkeys(self.b))\n",
        "  #   prompt = f'You are taking an English test.\\nHere is a question: \"{self.q}\"\\nHere is a python list of possible answers to the question: {self.b}\\nThe current python list may contain answers that are worded differently, but have the same meaning. Remove all the duplicate answers and return a python list of unique answers:'\n",
        "  #   self.b = str2lst(generateText([{'role': 'user', 'content': prompt}]))\n",
        "  #   return"
      ],
      "metadata": {
        "id": "oucXDIRuSR8j"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversartion Functions"
      ],
      "metadata": {
        "id": "WpDizLttaH8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def startConv(i_want_sen = 'I want to watch a movie'):\n",
        "  prompt = stichPrompt(prompts['startConv'][0], [i_want_sen])\n",
        "  # prompt = f\"\"\"You are taking an English test.\\nHere is a sentence: \"{i_want_sen}, but I do not know [insert word]. Help me by asking me a question at a time.\"\\nReturn the sentence but with the correct word filled in the empty slot:\"\"\"\n",
        "  prompt = generateText([{'role': 'user', 'content': prompt}]).strip(\"\"\" '\" \"\"\")\n",
        "\n",
        "  context = [{'role': 'user', 'content': prompt}]\n",
        "  # appendContext(generateText([{'role': 'user', 'content': prompt}]), context, 'assistant')\n",
        "\n",
        "  return context\n",
        "\n",
        "def continueConvBool(chat_hist):\n",
        "  assert chat_hist[-1]['role'] == 'assistant', 'chat history should have the assistant as the last message'\n",
        "  prompt = stichPrompt(prompts['continueConvBool'][0], [chat_hist[-1]['content']])\n",
        "\n",
        "  return 'question' in generateText([{'role': 'user', 'content': prompt}]).lower()"
      ],
      "metadata": {
        "id": "hcDKgTw0h7Aq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convStep(context):\n",
        "  return"
      ],
      "metadata": {
        "id": "N8C7ZAhqY41N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "99lqoLANlfyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = {'startConv': [\"\"\"You are taking an English test.\\nHere is a sentence: \"{i_want_sen}, but I do not know [insert word]. Help me by asking me a question at a time.\"\\nReturn the sentence but with the correct word filled in the empty slot:\"\"\"],\n",
        "\n",
        "           'genRealProfile': [\"Generate a profile of a user. The profile should be a list of keywords about their preferences regarding {domain}.\"],\n",
        "\n",
        "           'continueConvBool': [\"Is the following text a question or a suggestion?/n/n{text}/n/nAnswer [Question, Suggestion]:\"],\n",
        "\n",
        "           'genUProfiles': ['Generate {number} different profiles. Each profile should be one sentence containing a sequence of keywords about their preferences regarding {domain}.'],\n",
        "\n",
        "           'extractLastPreference': ['Here is a conversation.\\nASSISTANT:\\n{question}\\nUSER:\\n{answer}\\nTASK: Summarize what we know about the user in a single sentence.\\nSUMMARY:'],\n",
        "\n",
        "           'genYProfiles': ['Here is a list of user preferences:{bullet_points}',\n",
        "                            'Here is a sentence describing the preferences of a user:{sentence}\\n',\n",
        "                            'Create {number} different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:',\n",
        "                            'Create different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'],\n",
        "\n",
        "           'answerQ': ['You are a character. Here is what you know about your character: {profile}\\nYou are speaking to an assistant and you speak in brief sentences.\\nAnswer the assistant in character.\\nAssistant: {question}\\nYou:'],\n",
        "\n",
        "           'pickBranch': [\"\"\"You are the following character: {profile}\\nYou are asked the following question: {question}\\n\\nFrom the options below, which is most likely your answer to the question?\\n{bullet_points}\\nRemember, you have to pick one from the list or answer \"None of the answers in the list\". Only return the most likely answer, followed by the number that is the place of the answer in the list.\\nAnswer, Number:\"\"\",\n",
        "                          \"You are the following character: {profile}\\nYou are asked the following question: {question}\\nIs it likely that you would answer: {answer}?\\n\\n[Yes, No]:\"],\n",
        "\n",
        "          'makeBranches': ['Here is a question:\\n\"{question}\"\\nReturn the smallest numbered list of different possible answers to this question. Make sure that this list, althought small, covers how any user could answer this question.\\nList of answers:'],\n",
        "\n",
        "           'makeNodes': ['Generate a python list of {node_n} questions you could possibly ask me at this point of the conversation.',\n",
        "                         'Generate a python list of questions you could possibly ask me at this point of the conversation.',\n",
        "                         ' Inlcude the last question you just asked in the python list.',\n",
        "                         \"Let's say I said the following: {answer}\\n {prompt}\"],\n",
        "\n",
        "\n",
        "           }"
      ],
      "metadata": {
        "id": "6pr0ATXMM3wJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate u profiles\n",
        "u_profiles = genUProfiles(10)"
      ],
      "metadata": {
        "id": "4upRYruVN7Pa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the real profile\n",
        "real_profile = genRealProfile()"
      ],
      "metadata": {
        "id": "VpFmirFtbVuF"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(real_profile.text)\n",
        "pprint.pprint(real_profile.answerQ(real_profile, 'Do you have any specific actors or directors that you prefer?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdvsDF_Hczk4",
        "outputId": "378a32c1-0dd0-4dd5-bee6-2ec714b90945"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('1. Genre preferences: Action, Thriller, Sci-Fi, and Fantasy\\n'\n",
            " '2. Film style: Suspenseful, visually stunning, and mind-bending\\n'\n",
            " '3. Plot elements: Intricate twists, unexpected turns, and unpredictable '\n",
            " 'endings\\n'\n",
            " '4. Character types: Complex, morally ambiguous, and anti-heroes\\n'\n",
            " '5. Director preferences: Christopher Nolan, Quentin Tarantino, and Denis '\n",
            " 'Villeneuve\\n'\n",
            " '6. Acting style: Intense performances, method acting, and nuanced '\n",
            " 'portrayals\\n'\n",
            " '7. Cinematic themes: Existentialism, identity, and the human condition\\n'\n",
            " '8. Visual effects: Cutting-edge CGI, breathtaking cinematography, and '\n",
            " 'immersive world-building\\n'\n",
            " '9. Dialogue style: Sharp, witty, and thought-provoking exchanges\\n'\n",
            " '10. Soundtrack preferences: Epic orchestral scores, atmospheric soundscapes, '\n",
            " 'and memorable theme songs\\n'\n",
            " '11. Length preference: Prefer longer runtimes to fully explore intricate '\n",
            " 'storylines and character development\\n'\n",
            " '12. International cinema: Open to foreign films, especially those with '\n",
            " 'unique storytelling perspectives\\n'\n",
            " '13. Cult classics: Appreciates cult films with a dedicated fan base and '\n",
            " 'unconventional narratives\\n'\n",
            " '14. Award-winning films: Interested in critically acclaimed movies '\n",
            " 'recognized for their artistic merit and innovation\\n'\n",
            " '15. Franchise preference: Enjoys multi-film series with interconnected '\n",
            " 'storylines and character arcs.')\n",
            "('I prefer actors who can deliver intense performances and directors known for '\n",
            " 'their visually stunning and mind-bending films. Christopher Nolan, Quentin '\n",
            " 'Tarantino, and Denis Villeneuve are some of my top choices.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize layer\n",
        "layer = Layer(startConv())"
      ],
      "metadata": {
        "id": "TxSm2DCxKyM0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating nodes in layer, hence possible questions\n",
        "layer.makeNodes(3)"
      ],
      "metadata": {
        "id": "3YDZFlhPVul8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[n.question for n in layer.nodes]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSpmC99G0j2d",
        "outputId": "c1fb96f0-dc2e-4e53-84e9-b2f350f51b8b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What genre of movies do you usually enjoy watching?',\n",
              " 'Are you in the mood for a light-hearted comedy or a thrilling action-packed movie?',\n",
              " 'Do you have any specific actors or directors that you prefer?']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the branches for each of the nodes (without using profiles)\n",
        "layer.makeBranches()"
      ],
      "metadata": {
        "id": "xCN97XAHPlp7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint([n.branches for n in layer.nodes])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXUb2HHiNwZl",
        "outputId": "5657d3fd-e41f-42a3-ef24-b4e5f341a9a8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Action',\n",
            "  'Adventure',\n",
            "  'Comedy',\n",
            "  'Drama',\n",
            "  'Horror',\n",
            "  'Science Fiction',\n",
            "  'Thriller',\n",
            "  'Romance',\n",
            "  'Fantasy',\n",
            "  'Documentary'],\n",
            " ['Yes', 'No', 'Maybe'],\n",
            " ['Yes, I have specific actors and directors that I prefer.',\n",
            "  'No, I do not have any specific actors or directors that I prefer.',\n",
            "  'I have a few favorite actors and directors, but the list is not extensive.',\n",
            "  'While I dont have an absolute preference, there are some actors and '\n",
            "  'directors whose work I enjoy.',\n",
            "  'It varies for me, depending on the genre or type of film.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# layer.nodes[0].tst_b"
      ],
      "metadata": {
        "id": "5YPeTtHkP4gQ"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the profiles\n",
        "layer.profiles = u_profiles\n",
        "layer.splitProfiles()"
      ],
      "metadata": {
        "id": "dF6vW0jRBWvr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in layer.nodes: print(n.profile_branch_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NVZ5BnaGUxZ",
        "outputId": "6ed339a5-b78e-4a36-b60a-48c011351786"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 3, 7, 4, 8, 8, 2, 6, 10, 3]\n",
            "[3, 3, 0, 3, 0, 2, 0, 1, 3, 3]\n",
            "[2, 2, 2, 3, 2, 4, 3, 2, 4, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n_idx in range(len(layer.nodes)):\n",
        "  print(f'////////// {layer.nodes[n_idx].question} //////////\\n')\n",
        "  for p_idx in range(len(layer.profiles)):\n",
        "    print(f'{layer.profiles[p_idx].text}')\n",
        "    choice = layer.nodes[n_idx].profile_branch_map[p_idx]\n",
        "    print(f'{layer.nodes[n_idx].branches[choice]}\\n')\n",
        "    # if choice is not None: print(f'{layer.nodes[n_idx].branches[choice]}\\n')\n",
        "    # else: print('None\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0hHoh7MDv_s",
        "outputId": "31cf2a39-15f5-4d33-9646-7599cdd480a0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "////////// What genre of movies do you usually enjoy watching? //////////\n",
            "\n",
            "Profile 1: Jane is an avid moviegoer who enjoys action-packed blockbusters, science fiction thrillers, and mind-bending mysteries, with a soft spot for movies that feature strong female leads.\n",
            "Science Fiction\n",
            "\n",
            "Profile 2: John is a cinephile who gravitates towards independent films, foreign cinema, and thought-provoking documentaries that explore socio-political issues and human nature.\n",
            "Drama\n",
            "\n",
            "Profile 3: Sarah is a romantic at heart, always seeking out heartwarming love stories, charming romantic comedies, and emotionally charged dramas that tug at her heartstrings.\n",
            "Romance\n",
            "\n",
            "Profile 4: Michael is a fan of the horror genre, eagerly watching spine-chilling supernatural flicks, psychological thrillers, and gory slasher films that keep him on the edge of his seat.\n",
            "Horror\n",
            "\n",
            "Profile 5: Emily is a lover of animated movies, immersing herself in enchanting Disney classics, heartening Pixar animations, and visually stunning Japanese anime films.\n",
            "Fantasy\n",
            "\n",
            "Profile 6: David is an adventure enthusiast, drawn to epic fantasy films, thrilling adventure movies, and historical dramas that transport him to different worlds and time periods.\n",
            "Fantasy\n",
            "\n",
            "Profile 7: Melissa is a comedy aficionado, always in search of laugh-out-loud comedies, witty satires, and light-hearted feel-good movies that bring joy and laughter into her life.\n",
            "Comedy\n",
            "\n",
            "Profile 8: Ryan is a fan of gripping crime dramas, intricate suspenseful films, and gritty neo-noir thrillers that delve into the dark underbelly of society and captivate his imagination.\n",
            "Thriller\n",
            "\n",
            "Profile 9: Amanda is a documentary buff, immersing herself in enlightening true stories, captivating nature documentaries, and insightful social issue films that broaden her horizons.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Profile 10: Alex is a fan of historical movies, indulging in period dramas, biographical films, and war epics that shed light on significant events and personalities from the past.\n",
            "Drama\n",
            "\n",
            "////////// Are you in the mood for a light-hearted comedy or a thrilling action-packed movie? //////////\n",
            "\n",
            "Profile 1: Jane is an avid moviegoer who enjoys action-packed blockbusters, science fiction thrillers, and mind-bending mysteries, with a soft spot for movies that feature strong female leads.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Profile 2: John is a cinephile who gravitates towards independent films, foreign cinema, and thought-provoking documentaries that explore socio-political issues and human nature.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Profile 3: Sarah is a romantic at heart, always seeking out heartwarming love stories, charming romantic comedies, and emotionally charged dramas that tug at her heartstrings.\n",
            "Yes\n",
            "\n",
            "Profile 4: Michael is a fan of the horror genre, eagerly watching spine-chilling supernatural flicks, psychological thrillers, and gory slasher films that keep him on the edge of his seat.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Profile 5: Emily is a lover of animated movies, immersing herself in enchanting Disney classics, heartening Pixar animations, and visually stunning Japanese anime films.\n",
            "Yes\n",
            "\n",
            "Profile 6: David is an adventure enthusiast, drawn to epic fantasy films, thrilling adventure movies, and historical dramas that transport him to different worlds and time periods.\n",
            "Maybe\n",
            "\n",
            "Profile 7: Melissa is a comedy aficionado, always in search of laugh-out-loud comedies, witty satires, and light-hearted feel-good movies that bring joy and laughter into her life.\n",
            "Yes\n",
            "\n",
            "Profile 8: Ryan is a fan of gripping crime dramas, intricate suspenseful films, and gritty neo-noir thrillers that delve into the dark underbelly of society and captivate his imagination.\n",
            "No\n",
            "\n",
            "Profile 9: Amanda is a documentary buff, immersing herself in enlightening true stories, captivating nature documentaries, and insightful social issue films that broaden her horizons.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "Profile 10: Alex is a fan of historical movies, indulging in period dramas, biographical films, and war epics that shed light on significant events and personalities from the past.\n",
            "NONE OF THE ABOVE\n",
            "\n",
            "////////// Do you have any specific actors or directors that you prefer? //////////\n",
            "\n",
            "Profile 1: Jane is an avid moviegoer who enjoys action-packed blockbusters, science fiction thrillers, and mind-bending mysteries, with a soft spot for movies that feature strong female leads.\n",
            "I have a few favorite actors and directors, but the list is not extensive.\n",
            "\n",
            "Profile 2: John is a cinephile who gravitates towards independent films, foreign cinema, and thought-provoking documentaries that explore socio-political issues and human nature.\n",
            "I have a few favorite actors and directors, but the list is not extensive.\n",
            "\n",
            "Profile 3: Sarah is a romantic at heart, always seeking out heartwarming love stories, charming romantic comedies, and emotionally charged dramas that tug at her heartstrings.\n",
            "I have a few favorite actors and directors, but the list is not extensive.\n",
            "\n",
            "Profile 4: Michael is a fan of the horror genre, eagerly watching spine-chilling supernatural flicks, psychological thrillers, and gory slasher films that keep him on the edge of his seat.\n",
            "While I dont have an absolute preference, there are some actors and directors whose work I enjoy.\n",
            "\n",
            "Profile 5: Emily is a lover of animated movies, immersing herself in enchanting Disney classics, heartening Pixar animations, and visually stunning Japanese anime films.\n",
            "I have a few favorite actors and directors, but the list is not extensive.\n",
            "\n",
            "Profile 6: David is an adventure enthusiast, drawn to epic fantasy films, thrilling adventure movies, and historical dramas that transport him to different worlds and time periods.\n",
            "It varies for me, depending on the genre or type of film.\n",
            "\n",
            "Profile 7: Melissa is a comedy aficionado, always in search of laugh-out-loud comedies, witty satires, and light-hearted feel-good movies that bring joy and laughter into her life.\n",
            "While I dont have an absolute preference, there are some actors and directors whose work I enjoy.\n",
            "\n",
            "Profile 8: Ryan is a fan of gripping crime dramas, intricate suspenseful films, and gritty neo-noir thrillers that delve into the dark underbelly of society and captivate his imagination.\n",
            "I have a few favorite actors and directors, but the list is not extensive.\n",
            "\n",
            "Profile 9: Amanda is a documentary buff, immersing herself in enlightening true stories, captivating nature documentaries, and insightful social issue films that broaden her horizons.\n",
            "It varies for me, depending on the genre or type of film.\n",
            "\n",
            "Profile 10: Alex is a fan of historical movies, indulging in period dramas, biographical films, and war epics that shed light on significant events and personalities from the past.\n",
            "While I dont have an absolute preference, there are some actors and directors whose work I enjoy.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer.calcInfoGain()"
      ],
      "metadata": {
        "id": "W4RQSuQNERLM"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in layer.nodes:\n",
        "  print(n.info_gains)\n",
        "\n",
        "print(layer.nodes[layer.best_node_idx].question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6EZ6QCsk4Lp",
        "outputId": "f1e7a650-0b9e-409b-d846-8e7ec7175f5c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9219280948873623\n",
            "1.6854752972273341\n",
            "1.4854752972273342\n",
            "What genre of movies do you usually enjoy watching?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer.context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hra2eNaGXebO",
        "outputId": "096d29dc-f932-45be-9651-ce09c8d8c1aa"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user',\n",
              "  'content': 'I want to watch a movie, but I do not know which one. Help me by asking me a question at a time.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OLD CODE"
      ],
      "metadata": {
        "id": "lsb2udidn1NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_with_idk(options, p_idk = 0.2):\n",
        "  return random.choices([random.choice(options), \"I don't know\"], weights = [1-p_idk, p_idk], k=1)[0]"
      ],
      "metadata": {
        "id": "SOdIZFFGEca2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_branch(q, branches, profiles):\n",
        "  ans = []\n",
        "\n",
        "  for p in profiles:\n",
        "    prompt = f\"\"\"You are the following character: {p}\\nYou are asked the following question: {q}\\n\\nFrom the options below, which is most likely your answer to the question?\\n{list2numbered(branches)}\\nRemember, you have to pick one from the list. Only return the most likely answer, followed by the number that is the place of the answer in the list.\\nAnswer, Number:\"\"\"\n",
        "    txt = generateText([{'role': 'user', 'content': prompt}])\n",
        "    m = re.search('\\d', txt)\n",
        "    if m:\n",
        "      s = m.start()\n",
        "      e = m.end()\n",
        "      ans.append(int(txt[s:e]))\n",
        "    else: ans.append(0)\n",
        "\n",
        "\n",
        "  return ans"
      ],
      "metadata": {
        "id": "Bkkg1IiyELh9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genAnswP(profile, q):\n",
        "  prompt = f'You are a character. Here is what you know about your character: {profile}\\nYou are speaking to an assistant and you speak in brief sentences.\\nAnswer the assistant in character.\\nAssistant: {q}\\nYou:'\n",
        "  return generateText([{'role': 'user', 'content': prompt}])\n",
        "\n",
        "def genAns(q):\n",
        "  prompt = f'Here is a question:\\n\"{q}\"\\nReturn a python list with all the possible answers to the question:'\n",
        "  txt = generateText([{'role': 'user', 'content': prompt}], tries = 2)\n",
        "  return str2lst(txt)\n",
        "\n",
        "def aggregateAns(ans_lst, q = None):\n",
        "  print('aggregating')\n",
        "  ans_lst = list(dict.fromkeys(ans_lst))\n",
        "  #prompt = f'Here is a list of preferences: {ans_lst}\\nRemove all the answers that mean the same thing and return a list of unique answers:' BEST\n",
        "  # prompt = f'Here is a list of preferences: {ans_lst}\\nRemove all the answers that are too similar and return a small list of unique answers:'\n",
        "  if q:\n",
        "    #prompt = f'You will be given a python list that has possible answers to the question \"{q}\"\\nYour task is to remove all the answers that mean the same thing and return a small list of unique answers.\\nHere is the python list: {ans_lst}'\n",
        "    prompt = f'You are taking an English test.\\nHere is a question: \"{q}\"\\nHere is a python list of possible answers to the question: {ans_lst}\\nThe current python list contains answers that are worded differently, but contain the same meaning. Remove all the duplicate answers and return a python list of unique answers:'\n",
        "  else:\n",
        "    prompt = f'Here is a list of preferences: {ans_lst}\\nRemove all the answers that mean the same thing and return a list of unique answers:'\n",
        "\n",
        "  return str2lst(generateText([{'role': 'user', 'content': prompt}]))"
      ],
      "metadata": {
        "id": "ZPAfvOyd_sgv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class representing a group of profiles\n",
        "class ProfileGroup:\n",
        "  def __init__(self, preference_lst = None):\n",
        "    self.preference_lst = preference_lst\n",
        "    self.profiles = []\n",
        "    # if preference_lst:\n",
        "    #   self.profiles = self.makeYProfiles()\n",
        "    #   self.pType = ProfileType.Y\n",
        "    # else:\n",
        "    #   self.profiles = self.makeUProfiles()\n",
        "    #   self.pType = ProfileType.U\n",
        "    return\n",
        "\n",
        "  # each profile answers question q, returns list of strings\n",
        "  def answerQ(self, q):\n",
        "    return multiThread(Profile.answerQ, [[p, self.q] for p in self.profiles])\n",
        "\n",
        "  # each profile picks branch that is best fit, returns list of int\n",
        "  def pickBranch(self, node):\n",
        "    return multiThread(Profile.pickBranch, [[p, node] for p in self.profiles])\n",
        "\n",
        "  def makeUProfiles(self, prof_num):\n",
        "    prompt = f'Generate {prof_num} different profiles. Each profile should be one sentence containing a sequence of keywords about their preferences regarding traveling.'\n",
        "    txt = generateText([{'role': 'user', 'content': prompt}], tries = 2)\n",
        "    self.profiles.extend(str2lst(txt))\n",
        "    return\n",
        "\n",
        "  def makeYProfiles(self):\n",
        "    p_n = len(self.preference_lst)\n",
        "    prompt = self.preference_lst[0]\n",
        "\n",
        "    # if preference_lst has more than one preferece, create a single sentence representing all of them\n",
        "    if p_n > 1:\n",
        "      prompt = f'Here is a list of user preferences:{list2numbered(self.preference_lst)}'\n",
        "      prompt = generateText([{'role': 'user', 'content': prompt+'Create a single sentence of key words that describes all of these preferences:'}], tries = 2)\n",
        "\n",
        "    prompt = f'Here is a sentence describing the preferences of a user:{prompt}\\n'\n",
        "\n",
        "    # if profile number is specified, generate this specific number, otherwise let LLM decide\n",
        "    prompt += f'Create {self.prof_num} different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'\n",
        "    # prompt += 'Create different versions of this sentence, where you add many new extra preferences on top that the user might have. Each sentence should have the known preferences and also have different imagined preferences from each other:'\n",
        "\n",
        "    return str2lst(generateText([{'role': 'user', 'content': prompt}], tries = 2))"
      ],
      "metadata": {
        "id": "Dh8P2VoV0XRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs_ans = [[] for q in range(len(p_qs))]\n",
        "for idx, a in enumerate(ans): qs_ans[idx%len(p_qs)].append(a)\n",
        "print(len(qs_ans))\n",
        "print(len(qs_ans[0]))"
      ],
      "metadata": {
        "id": "ufNSNJfmjonl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs_ans[0]"
      ],
      "metadata": {
        "id": "phATC3i17x24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aggregating answers\n",
        "branches = [[] for q in range(len(p_qs))]\n",
        "for i, ans in enumerate(qs_ans):\n",
        "  branches[i] = aggregateAns(ans)\n",
        "\n",
        "for b in branches:\n",
        "  print(len(b))\n",
        "pprint.pprint(branches)"
      ],
      "metadata": {
        "id": "JXc9psnpvNIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "branches = [[] for q in range(len(p_qs))]\n",
        "for i, ans in enumerate(qs_ans):\n",
        "  branches[i] = aggregateAns(ans, p_qs[i])\n",
        "\n",
        "for b in branches:\n",
        "  print(len(b))\n",
        "pprint.pprint(branches)"
      ],
      "metadata": {
        "id": "LDrBGPj8MRCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}